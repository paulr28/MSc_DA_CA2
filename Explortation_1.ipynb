{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import praw\n",
    "from textblob import TextBlob\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import wilcoxon\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "from jupyter_dash import JupyterDash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD 1980-Q1  1980-Q2  1980-Q3   \\\n",
      "0                       Q,I2015,NSA,IS-EPI,F,AL       :        :        :    \n",
      "1                       Q,I2015,NSA,IS-EPI,F,AT       :        :        :    \n",
      "2                       Q,I2015,NSA,IS-EPI,F,BE       :        :        :    \n",
      "3                       Q,I2015,NSA,IS-EPI,F,BG       :        :        :    \n",
      "4                       Q,I2015,NSA,IS-EPI,F,CH       :        :        :    \n",
      "\n",
      "  1980-Q4  1981-Q1  1981-Q2  1981-Q3  1981-Q4  1982-Q1   ... 2020-Q4   \\\n",
      "0       :        :        :        :        :        :   ...   119.4    \n",
      "1       :        :        :        :        :        :   ...   116.9    \n",
      "2       :        :        :        :        :        :   ...   108.2    \n",
      "3       :        :        :        :        :        :   ...   103.3    \n",
      "4       :        :        :        :        :        :   ...  102.1 e   \n",
      "\n",
      "  2021-Q1  2021-Q2  2021-Q3  2021-Q4  2022-Q1  2022-Q2  2022-Q3  2022-Q4   \\\n",
      "0   123.7    123.8    122.6    121.6    129.5    126.3    127.0    124.4    \n",
      "1   110.8    122.6    124.7    119.9    115.5    125.0    126.5    121.7    \n",
      "2   109.1    113.0    113.9   114.3 p  115.8 p  116.0 p  116.3 p      : c   \n",
      "3   94.5 p   94.9 p   92.8 p   91.9 p   92.0 p   90.9 p   89.1 p   88.9 p   \n",
      "4  101.4 e  103.2 e  105.8 e  103.9 e  104.5 e  105.3 e  106.8 e  106.5 e   \n",
      "\n",
      "  2023-Q1   \n",
      "0       :   \n",
      "1  113.6 p  \n",
      "2       :   \n",
      "3       :   \n",
      "4  106.1 e  \n",
      "\n",
      "[5 rows x 174 columns]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/EI_ISBU_Q?format=TSV&compressed=true' # The url of the data\n",
    "headers = {'Accept-Encoding': 'gzip'} # This is important to get the gzip file\n",
    "response = requests.get(url, headers=headers) # Get the data from the url\n",
    "\n",
    "buf = BytesIO(response.content) # Read the gzip file\n",
    "f = gzip.GzipFile(fileobj=buf) # Unzip the gzip file\n",
    "content = f.read() # Read the unzipped file\n",
    "\n",
    "df = pd.read_csv(BytesIO(content), sep='\\t') # Read the unzipped file as a dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>unit</th>\n",
       "      <th>s_adj</th>\n",
       "      <th>indic</th>\n",
       "      <th>nace_r2</th>\n",
       "      <th>geo\\TIME_PERIOD</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>AL</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>AT</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>BE</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>BG</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>CH</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  freq   unit s_adj   indic nace_r2 geo\\TIME_PERIOD   Quarter Value\n",
       "0    Q  I2015   NSA  IS-EPI       F              AL  1980-Q1     : \n",
       "1    Q  I2015   NSA  IS-EPI       F              AT  1980-Q1     : \n",
       "2    Q  I2015   NSA  IS-EPI       F              BE  1980-Q1     : \n",
       "3    Q  I2015   NSA  IS-EPI       F              BG  1980-Q1     : \n",
       "4    Q  I2015   NSA  IS-EPI       F              CH  1980-Q1     : "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Melt the dataframe so all the Quarters are in one column\n",
    "df = pd.melt(df, id_vars=['freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD'], var_name='Quarter', value_name='Value')\n",
    "\n",
    "# The column freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD is split into 6 columns\n",
    "df[['freq','unit','s_adj','indic','nace_r2','geo\\TIME_PERIOD']] = df['freq,unit,s_adj,indic,nace_r2,geo\\\\TIME_PERIOD'].str.split(',', expand=True)\n",
    "\n",
    "# Drop the column freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD and move Quarter and Value to the back\n",
    "df = df.drop(['freq,unit,s_adj,indic,nace_r2,geo\\\\TIME_PERIOD'], axis=1)\n",
    "df = df[['freq', 'unit', 's_adj', 'indic', 'nace_r2','geo\\TIME_PERIOD','Quarter', 'Value']]\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70930 entries, 0 to 70929\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   freq             70930 non-null  object\n",
      " 1   unit             70930 non-null  object\n",
      " 2   s_adj            70930 non-null  object\n",
      " 3   indic            70930 non-null  object\n",
      " 4   nace_r2          70930 non-null  object\n",
      " 5   geo\\TIME_PERIOD  70930 non-null  object\n",
      " 6   Quarter          70930 non-null  object\n",
      " 7   Value            70930 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\AppData\\Local\\Temp\\ipykernel_9012\\2627470230.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  df.loc[df['Value'].str.contains(':'), 'Value'] = pd.np.nan\n"
     ]
    }
   ],
   "source": [
    "# set values containing \":\" to NaN\n",
    "df.loc[df['Value'].str.contains(':'), 'Value'] = pd.np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\AppData\\Local\\Temp\\ipykernel_9012\\2254030720.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Value'] = df['Value'].str.replace('[a-zA-Z]', '')\n"
     ]
    }
   ],
   "source": [
    "# if the Value column has an alphabetical character remove it\n",
    "df['Value'] = df['Value'].str.replace('[a-zA-Z]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Convert the Value column to a float\n",
    "df['Value'] = df['Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>unit</th>\n",
       "      <th>s_adj</th>\n",
       "      <th>indic</th>\n",
       "      <th>nace_r2</th>\n",
       "      <th>geo\\TIME_PERIOD</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>AL</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>AT</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>BE</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>BG</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>NSA</td>\n",
       "      <td>IS-EPI</td>\n",
       "      <td>F</td>\n",
       "      <td>CH</td>\n",
       "      <td>1980-Q1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  freq   unit s_adj   indic nace_r2 geo\\TIME_PERIOD   Quarter  Value\n",
       "0    Q  I2015   NSA  IS-EPI       F              AL  1980-Q1     NaN\n",
       "1    Q  I2015   NSA  IS-EPI       F              AT  1980-Q1     NaN\n",
       "2    Q  I2015   NSA  IS-EPI       F              BE  1980-Q1     NaN\n",
       "3    Q  I2015   NSA  IS-EPI       F              BG  1980-Q1     NaN\n",
       "4    Q  I2015   NSA  IS-EPI       F              CH  1980-Q1     NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "countrycode_map = {c.alpha_2: c.alpha_3 for c in pycountry.countries} # Create a dictionary of country codes\n",
    "\n",
    "#Uk is not a country code so we will change it to GB\n",
    "df.loc[df['geo\\\\TIME_PERIOD'] == 'UK', 'geo\\\\TIME_PERIOD'] = 'GB'\n",
    "\n",
    "df['Country_Codes'] = df['geo\\\\TIME_PERIOD'].map(countrycode_map) # Map the country codes to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALB' 'AUT' 'BEL' 'BGR' 'CHE' 'CYP' 'CZE' 'DEU' 'DNK' nan 'EST' 'ESP'\n",
      " 'FIN' 'FRA' 'HRV' 'HUN' 'IRL' 'ISL' 'ITA' 'LTU' 'LUX' 'LVA' 'MNE' 'MKD'\n",
      " 'MLT' 'NLD' 'NOR' 'POL' 'PRT' 'ROU' 'SRB' 'SWE' 'SVN' 'SVK' 'TUR' 'GBR'\n",
      " 'BIH']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of the Country_Codes column\n",
    "print(df['Country_Codes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Perform Normailty tests for the Value column grouped by geo\\TIME_PERIOD, indic, and s_adj\n",
    "\n",
    "df.set_index(['geo\\\\TIME_PERIOD', 'indic', 's_adj'], inplace=True)\n",
    "\n",
    "# Perform normality test on the Value column grouped by geo\\TIME_PERIOD, indic, and s_adj\n",
    "grouped = df.groupby(['geo\\\\TIME_PERIOD', 'indic', 's_adj'])['Value']\n",
    "pvalues = grouped.apply(lambda x: normaltest(x.dropna())[1])\n",
    "\n",
    "# Add p-values to a new column in the original DataFrame\n",
    "df.loc[pvalues.index, 'pvalue'] = pvalues\n",
    "\n",
    "#Add a new column to the dfi dataframe called 'normal' and set it to True if the pvalue is greater than 0.05 and False if it is less than 0.05\n",
    "df['normal'] = df['pvalue'] > 0.05\n",
    "\n",
    "# Move the index back to columns\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create a country plot with the Value column as the color and the Quarter column as the animation, show for indicators '[IS-IP]' and Seasonal Adjustment 'NSA'\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39;49mchoropleth(df[(df[\u001b[39m'\u001b[39;49m\u001b[39mindic\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mIS-IP\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m&\u001b[39;49m (df[\u001b[39m'\u001b[39;49m\u001b[39ms_adj\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mNSA\u001b[39;49m\u001b[39m'\u001b[39;49m)], locations\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCountry_Codes\u001b[39;49m\u001b[39m\"\u001b[39;49m,color\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mValue\u001b[39;49m\u001b[39m\"\u001b[39;49m, hover_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgeo\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTIME_PERIOD\u001b[39;49m\u001b[39m\"\u001b[39;49m, animation_frame\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mQuarter\u001b[39;49m\u001b[39m\"\u001b[39;49m, color_continuous_scale\u001b[39m=\u001b[39;49mpx\u001b[39m.\u001b[39;49mcolors\u001b[39m.\u001b[39;49msequential\u001b[39m.\u001b[39;49mPlasma, range_color\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m100\u001b[39;49m), scope\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39meurope\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# Create the plot\u001b[39;00m\n\u001b[0;32m      5\u001b[0m fig\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\plotly\\express\\_chart_types.py:1091\u001b[0m, in \u001b[0;36mchoropleth\u001b[1;34m(data_frame, lat, lon, locations, locationmode, geojson, featureidkey, color, facet_row, facet_col, facet_col_wrap, facet_row_spacing, facet_col_spacing, hover_name, hover_data, custom_data, animation_frame, animation_group, category_orders, labels, color_discrete_sequence, color_discrete_map, color_continuous_scale, range_color, color_continuous_midpoint, projection, scope, center, fitbounds, basemap_visible, title, template, width, height)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchoropleth\u001b[39m(\n\u001b[0;32m   1052\u001b[0m     data_frame\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1053\u001b[0m     lat\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     height\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1086\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m go\u001b[39m.\u001b[39mFigure:\n\u001b[0;32m   1087\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m \u001b[39m    In a choropleth map, each row of `data_frame` is represented by a\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[39m    colored region mark on a map.\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1091\u001b[0m     \u001b[39mreturn\u001b[39;00m make_figure(\n\u001b[0;32m   1092\u001b[0m         args\u001b[39m=\u001b[39;49m\u001b[39mlocals\u001b[39;49m(),\n\u001b[0;32m   1093\u001b[0m         constructor\u001b[39m=\u001b[39;49mgo\u001b[39m.\u001b[39;49mChoropleth,\n\u001b[0;32m   1094\u001b[0m         trace_patch\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(locationmode\u001b[39m=\u001b[39;49mlocationmode),\n\u001b[0;32m   1095\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\plotly\\express\\_core.py:2213\u001b[0m, in \u001b[0;36mmake_figure\u001b[1;34m(args, constructor, trace_patch, layout_patch)\u001b[0m\n\u001b[0;32m   2211\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   2212\u001b[0m     layout_patch[\u001b[39m\"\u001b[39m\u001b[39mtitle_text\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m args[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 2213\u001b[0m \u001b[39melif\u001b[39;00m args[\u001b[39m\"\u001b[39;49m\u001b[39mtemplate\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mlayout\u001b[39m.\u001b[39;49mmargin\u001b[39m.\u001b[39mt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2214\u001b[0m     layout_patch[\u001b[39m\"\u001b[39m\u001b[39mmargin\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m60\u001b[39m}\n\u001b[0;32m   2215\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2216\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m args\n\u001b[0;32m   2217\u001b[0m     \u001b[39mand\u001b[39;00m args[\u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2218\u001b[0m     \u001b[39mand\u001b[39;00m args[\u001b[39m\"\u001b[39m\u001b[39mtemplate\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlayout\u001b[39m.\u001b[39mlegend\u001b[39m.\u001b[39mitemsizing \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2219\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\plotly\\graph_objs\\_layout.py:2212\u001b[0m, in \u001b[0;36mLayout.margin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2180\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   2181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmargin\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   2182\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2183\u001b[0m \u001b[39m    The 'margin' property is an instance of Margin\u001b[39;00m\n\u001b[0;32m   2184\u001b[0m \u001b[39m    that may be specified as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2210\u001b[0m \u001b[39m    plotly.graph_objs.layout.Margin\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\plotly\\basedatatypes.py:5823\u001b[0m, in \u001b[0;36mBaseLayoutType.__getitem__\u001b[1;34m(self, prop)\u001b[0m\n\u001b[0;32m   5819\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5820\u001b[0m \u001b[39mCustom __getitem__ that handles dynamic subplot properties\u001b[39;00m\n\u001b[0;32m   5821\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5822\u001b[0m prop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strip_subplot_suffix_of_1(prop)\n\u001b[1;32m-> 5823\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(BaseLayoutHierarchyType, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(prop)\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\plotly\\basedatatypes.py:4699\u001b[0m, in \u001b[0;36mBasePlotlyType.__getitem__\u001b[1;34m(self, prop)\u001b[0m\n\u001b[0;32m   4694\u001b[0m \u001b[39mif\u001b[39;00m prop \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_props:\n\u001b[0;32m   4695\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_on_invalid_property_error(_error_to_raise\u001b[39m=\u001b[39mPlotlyKeyError)(\n\u001b[0;32m   4696\u001b[0m         prop\n\u001b[0;32m   4697\u001b[0m     )\n\u001b[1;32m-> 4699\u001b[0m validator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_validator(prop)\n\u001b[0;32m   4701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(validator, CompoundValidator):\n\u001b[0;32m   4702\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compound_props\u001b[39m.\u001b[39mget(prop, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4703\u001b[0m         \u001b[39m# Init compound objects\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\plotly\\basedatatypes.py:4320\u001b[0m, in \u001b[0;36mBasePlotlyType._get_validator\u001b[1;34m(self, prop)\u001b[0m\n\u001b[0;32m   4317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_validator\u001b[39m(\u001b[39mself\u001b[39m, prop):\n\u001b[0;32m   4318\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidator_cache\u001b[39;00m \u001b[39mimport\u001b[39;00m ValidatorCache\n\u001b[1;32m-> 4320\u001b[0m     \u001b[39mreturn\u001b[39;00m ValidatorCache\u001b[39m.\u001b[39;49mget_validator(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path_str, prop)\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\plotly\\validator_cache.py:28\u001b[0m, in \u001b[0;36mValidatorCache.get_validator\u001b[1;34m(parent_path, prop_name)\u001b[0m\n\u001b[0;32m     26\u001b[0m         lookup_name \u001b[39m=\u001b[39m lookup_name \u001b[39mor\u001b[39;00m prop_name\n\u001b[0;32m     27\u001b[0m         class_name \u001b[39m=\u001b[39m lookup_name\u001b[39m.\u001b[39mtitle() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mValidator\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 28\u001b[0m         validator \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\n\u001b[0;32m     29\u001b[0m             importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39mplotly.validators.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m parent_path),\n\u001b[0;32m     30\u001b[0m             class_name,\n\u001b[0;32m     31\u001b[0m         )(plotly_name\u001b[39m=\u001b[39mprop_name)\n\u001b[0;32m     32\u001b[0m     ValidatorCache\u001b[39m.\u001b[39m_cache[key] \u001b[39m=\u001b[39m validator\n\u001b[0;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m ValidatorCache\u001b[39m.\u001b[39m_cache[key]\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\site-packages\\_plotly_utils\\importers.py:36\u001b[0m, in \u001b[0;36mrelative_import.<locals>.__getattr__\u001b[1;34m(import_name)\u001b[0m\n\u001b[0;32m     34\u001b[0m     rel_module \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(rel_path_parts[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     35\u001b[0m     class_name \u001b[39m=\u001b[39m import_name\n\u001b[1;32m---> 36\u001b[0m     class_module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(rel_module, parent_name)\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(class_module, class_name)\n\u001b[0;32m     39\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{__name__!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{name!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     41\u001b[0m         name\u001b[39m=\u001b[39mimport_name, \u001b[39m__name__\u001b[39m\u001b[39m=\u001b[39mparent_name\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a country plot with the Value column as the color and the Quarter column as the animation, show for indicators '[IS-IP]' and Seasonal Adjustment 'NSA'\n",
    "\n",
    "fig = px.choropleth(df[(df['indic'] == 'IS-IP') & (df['s_adj'] == 'NSA')], locations=\"Country_Codes\",color=\"Value\", hover_name=\"geo\\TIME_PERIOD\", animation_frame=\"Quarter\", color_continuous_scale=px.colors.sequential.Plasma, range_color=(0, 100), scope='europe') # Create the plot\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by geo\\TIME_PERIOD, indic, and s_adj and show descriptive statistics\n",
    "df.groupby(['s_adj', 'indic', 'geo\\\\TIME_PERIOD']).describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the geo\\TIME_PERIOD value that equals 'IE' into a seperate dataframe\n",
    "df_ie = df[df['geo\\TIME_PERIOD'] == 'IE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform EDA on the IE dataframe\n",
    "df_ie.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ie.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Statistical Analysis on the IE dataframe\n",
    "df_ie.describe(include='all')\n",
    "\n",
    "df_ie.groupby(['indic','s_adj']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histomgrams of the Value column for the IE dataframe grouped by 'indic' and 's_adj', faceted by the 'normal' column\n",
    "sns.FacetGrid(df_ie, col='indic', row='s_adj', hue='normal').map(sns.histplot, 'Value').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms of the Value column for the IE dataframe grouped by 'indic' and 's_adj', where the 'normal' column is True\n",
    "sns.FacetGrid(df_ie[df_ie['normal'] == True], row='indic', col='s_adj', hue= 's_adj').map(sns.histplot, 'Value').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the IE dataframe showing statistical analysis using seaborn  and matplotlib  libraries splitting the data by insdicators and seasonanal adjustment\n",
    "sns.catplot(x=\"indic\", y=\"Value\", hue=\"s_adj\", kind=\"box\", data=df_ie);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the unique geo\\TIME_PERIOD values from the df dataframe, where the indic column equals 'IS-EPI' and the normal column equals True\n",
    "df[(df['indic'] == 'IS-EPI') & (df['normal'] == True)]['geo\\\\TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Ireland with 'EU28' which is the overall EU average for 28 countries\n",
    "\n",
    "# Create a dataframe called df_ie_eu28 that contains the geo\\TIME_PERIOD values of 'IE' and 'EU28'\n",
    "df_ie_eu28 = df[(df['geo\\\\TIME_PERIOD'] == 'IE') | (df['geo\\\\TIME_PERIOD'] == 'EU28')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Number of persons employed index, non-seasonally adjusted, for Ireland and EU28 using a t-test\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Filter the DataFrame to only include rows with geo\\TIME_PERIOD values of 'EU28' and 'IE',\n",
    "# 'IS-EPI' for indic, and 'NSA' for s_adj\n",
    "df_filtered = df[(df['geo\\\\TIME_PERIOD'].isin(['EU28', 'IE'])) & (df['indic'] == 'IS-EPI') & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Split the DataFrame into two separate DataFrames, one for each geo\\TIME_PERIOD value\n",
    "df_eu281 = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == 'EU28']\n",
    "df_ie1 = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == 'IE']\n",
    "\n",
    "# Perform the t-test\n",
    "t_stat, p_val = ttest_ind(df_eu281['Value'], df_ie1['Value'], equal_var=False)\n",
    "\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a One-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include rows where the normal column equals True and the indic column equals 'IS-EPI' and the s_adj column equals 'NSA'\n",
    "df_filtered = df[(df['normal'] == True) & (df['indic'] == 'IS-EPI') & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Create a list of the unique geo\\TIME_PERIOD values\n",
    "geo_time_periods = df_filtered['geo\\\\TIME_PERIOD'].unique()\n",
    "\n",
    "#Make sure the groups have the same sample size\n",
    "min_sample_size = df_filtered['geo\\\\TIME_PERIOD'].value_counts().min()\n",
    "df_filtered = df_filtered.groupby('geo\\\\TIME_PERIOD').apply(lambda x: x.sample(min_sample_size))\n",
    "\n",
    "#Perform the ANOVA\n",
    "f_stat, p_val = f_oneway(*[df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == geo_time_period]['Value'] for geo_time_period in geo_time_periods])\n",
    "\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "#Print th resuts of the ANOVA test\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject null hypothesis - Significant differences exist between groups.\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - No significant difference between groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the diferences between the groups using a boxplot with slanted x-axis labels\n",
    "sns.catplot(x=\"geo\\\\TIME_PERIOD\", y=\"Value\", kind=\"box\", data=df_filtered).set_xticklabels(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Tukey's Range Test to determine which groups are significantly different from each other\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's Range Test\n",
    "tukey_results = pairwise_tukeyhsd(df_filtered['Value'], df_filtered['geo\\\\TIME_PERIOD'], 0.05)\n",
    "\n",
    "# Print the results\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Two-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe called prep1 where the indic column equals 'IS-EPI' and the s_adj column contains 'SCA and 'NSA' values and the normal column equals True\n",
    "prep1 = df[(df['indic'] == 'IS-EPI') & (df['normal'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the name of the geo\\TIME_PERIOD column to geo in the df dataframe\n",
    "prep1.rename(columns={'geo\\\\TIME_PERIOD': 'geo'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a TWO-WAY ANOVA to determine if there is an interaction between the geo\\TIME_PERIOD and s_adj columns\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Value ~ s_adj + geo', data = prep1).fit()\n",
    "aov2 = sm.stats.anova_lm(model, type=2)\n",
    "print(aov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Wilcoxon Signed-Rank Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find countries that have false values for the normal column where the indic column equals 'IS-HWI'\n",
    "df[(df['normal'] == False) & (df['indic'] == 'IS-HWI')]['geo\\\\TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_HWI = df[(df['indic'] == 'IS-HWI')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_HWI = df_HWI.dropna(subset=['Value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the two groups to compare\n",
    "group1 = df_HWI[df_HWI['geo\\\\TIME_PERIOD'] == 'IE']['Value']\n",
    "group2 = df_HWI[df_HWI['geo\\\\TIME_PERIOD'] == 'EU28']['Value']\n",
    "\n",
    "# Ensure the two groups have the same sample size\n",
    "min_sample_size = min(len(group1), len(group2))\n",
    "group1 = group1.sample(min_sample_size)\n",
    "group2 = group2.sample(min_sample_size)\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p = wilcoxon(group1, group2)\n",
    "\n",
    "# Print the results\n",
    "print('Wilcoxon signed-rank test:')\n",
    "print(f'statistic: {stat:.4f}')\n",
    "print(f'p-value: {p:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Kruskall Wallis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "stat, p = kruskal(*[df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == geo]['Value'] for geo in df_filtered['geo\\\\TIME_PERIOD'].unique()])\n",
    "\n",
    "# Print the results\n",
    "print(\"Kruskal-Wallis Test Results:\")\n",
    "print(f\"Test statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the environmental variables\n",
    "APP_NAME = getenv('APP_NAME')\n",
    "APP_ID = getenv(\"APP_ID\")\n",
    "APP_SECRET = getenv(\"APP_SECRET\")\n",
    "USERNAME = getenv('REDDIT_USERNAME')\n",
    "PASSWORD = getenv('PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=APP_ID,\n",
    "    client_secret=APP_SECRET,\n",
    "    user_agent=APP_NAME,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    ")\n",
    "\n",
    "# Define the subreddits and search query\n",
    "subreddits = [\"Ireland\", \"Europe\"]\n",
    "query = \"house prices\"\n",
    "\n",
    "# Collect posts from the subreddits related to the search query\n",
    "posts = []\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for post in subreddit.search(query):\n",
    "        posts.append(\n",
    "            {\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"title\": post.title,\n",
    "                \"text\": post.selftext,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the collected posts into a dataframe\n",
    "df_posts = pd.DataFrame(posts)\n",
    "\n",
    "# Perform sentiment analysis on the collected posts\n",
    "df_posts[\"polarity\"] = df_posts[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Group the results by subreddit and calculate the mean polarity\n",
    "results = df_posts.groupby(\"subreddit\")[\"polarity\"].mean()\n",
    "\n",
    "# Print the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the results of the sentiment analysis\n",
    "if results[\"Ireland\"] > results[\"Europe\"]:\n",
    "    print(\"The sentiment of the posts from r/Ireland is more positive than the sentiment of the posts from r/Europe.\")\n",
    "elif results[\"Ireland\"] < results[\"Europe\"]:\n",
    "    print(\"The sentiment of the posts from r/Ireland is more negative than the sentiment of the posts from r/Europe.\")\n",
    "else:\n",
    "    print(\"The sentiment of the posts from r/Ireland is the same as the sentiment of the posts from r/Europe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Analysis\n",
    "\n",
    "# Create a dataframe called df_filtered where the indic column equals 'IS-HWI' and the s_adj column contains 'NSA' values and the normal column equals False\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA') & (df['geo\\\\TIME_PERIOD'] == 'IE')]\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Define a function to parse the date string\n",
    "def parse_quarter(date_string):\n",
    "    year, quarter = date_string.split('-Q')\n",
    "    month = (int(quarter) - 1) * 3 + 1\n",
    "    return datetime(int(year), month, 1)\n",
    "\n",
    "#Apply the parse_quarter function to the 'Quarter' column\n",
    "df_filtered['Quarter'] = df_filtered['Quarter'].apply(parse_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'Quarter' column as the index\n",
    "df_filtered.set_index('Quarter', inplace=True)\n",
    "\n",
    "# Create a time series plot of the 'Value' column\n",
    "df_filtered['Value'].plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Hours Worked Index in Ireland')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.asfreq('QS-OCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "# Perform time series analysis using SARIMAX\n",
    "# Split the data into train and test sets\n",
    "train = df_filtered.iloc[:len(df_filtered) - 4] # everything up to the last 4 observations\n",
    "test = df_filtered.iloc[len(df_filtered) - 4:]  # the last 4 observations\n",
    "\n",
    "# Create a SARIMAX model\n",
    "model = sm.tsa.statespace.SARIMAX(train['Value'])\n",
    "fit_model = model.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = fit_model.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions.plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions for the last 4 quarters')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test['Value'], predictions))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test['Value'], predictions, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import product\n",
    "# Tune the models hyperparameters to improve the RMSE\n",
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))\n",
    "\n",
    "# Find the optimal set of parameters that yields the best performance\n",
    "# Define the initial parameters\n",
    "best_score, best_params, best_seasonal_params = float(\"inf\"), None, None\n",
    "\n",
    "# Loop through the parameter combinations\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            # Create a SARIMAX model\n",
    "            model = sm.tsa.statespace.SARIMAX(train['Value'], order=param, seasonal_order=param_seasonal)\n",
    "\n",
    "            # Fit the model\n",
    "            results = model.fit()\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = results.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "            # Calculate the mean squared error\n",
    "            mse = mean_squared_error(test['Value'], predictions)\n",
    "\n",
    "            # If the mse is lower than our best score, update the best score, and best parameters\n",
    "            if mse < best_score:\n",
    "                best_score, best_params, best_seasonal_params = mse, param, param_seasonal\n",
    "\n",
    "            # Print the model parameters and the mean squared error\n",
    "            print('SARIMA{}x{}4 - AIC:{}'.format(param, param_seasonal, mse))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "# Print the best model parameters and the mean squared error\n",
    "print('Best SARIMA{}x{}4 AIC:{}'.format(best_params, best_seasonal_params, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SARIMAX model\n",
    "model1 = sm.tsa.statespace.SARIMAX(train['Value'], order=(1, 0, 0), seasonal_order=(0, 0, 1, 4))\n",
    "fit_model1 = model1.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions1 = fit_model1.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions1)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions1.plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions for the last 4 quarters')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test['Value'], predictions))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test['Value'], predictions, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the model\n",
    "model = sm.tsa.statespace.SARIMAX(train['Value'],order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "results = model.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = fit_model.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions.plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions for the last 4 quarters')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test['Value'], predictions))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test['Value'], predictions, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "results.plot_diagnostics(figsize=(12, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "pred = results.predict()\n",
    "\n",
    "# Plot the actual values and the predicted values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(df_filtered['Value'], label='Actual')\n",
    "ax.plot(pred, label='Predicted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction for the next 4 quarters\n",
    "pred = results.predict(start=len(df_filtered)-4, end=len(df_filtered) + 3)\n",
    "\n",
    "# Print the predictions\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Current Year with actual and predicted values and the Next Year with predicted values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(df_filtered['Value'], label='Actual')\n",
    "ax.plot(pred, label='Predicted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Analysis across countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe called df_filtered where the indic column equals 'IS-HWI' and the s_adj column contains 'NSA' values and the normal column equals False\n",
    "df_filteredall = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')]\n",
    "df_filteredall = df_filteredall.dropna(subset=['Value'])\n",
    "\n",
    "# Define a function to parse the date string\n",
    "def parse_quarter(date_string):\n",
    "    year, quarter = date_string.split('-Q')\n",
    "    month = (int(quarter) - 1) * 3 + 1\n",
    "    return datetime(int(year), month, 1)\n",
    "\n",
    "#Apply the parse_quarter function to the 'Quarter' column\n",
    "df_filteredall['Quarter'] = df_filteredall['Quarter'].apply(parse_quarter)\n",
    "\n",
    "# Set the style of the plots\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Use the 'FacetGrid' function to create facetted plots\n",
    "g = sns.FacetGrid(df_filteredall, col='Country_Codes', col_wrap=3, height=4)\n",
    "\n",
    "# Map the line plot onto the facetted plots\n",
    "g.map(sns.lineplot, 'Quarter', 'Value')\n",
    "\n",
    "# Set the x-axis label for each plot\n",
    "g.set_axis_labels('Quarter', 'Value')\n",
    "\n",
    "# Set the title for each plot\n",
    "g.set_titles('{col_name}')\n",
    "\n",
    "# Adjust the spacing between the plots\n",
    "g.tight_layout()\n",
    "\n",
    "# Show the facetted plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index the dataframe by the 'Quarter' column\n",
    "df_filtered.set_index('Quarter', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sarimax_model(data):\n",
    "    # Split the data into train and test sets (e.g., 80% train, 20% test)\n",
    "    train_data = data[:-4]  # Use all but the last 4 quarters for training\n",
    "    test_data = data[-4:]  # Use the last 4 quarters for testing\n",
    "    \n",
    "    # Fit the SARIMAX model\n",
    "    model = sm.tsa.statespace.SARIMAX(train_data, order=(1, 0, 0), seasonal_order=(1, 0, 0, 4))\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Make predictions for the test set\n",
    "    predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
    "\n",
    "    # Return the model fit and predictions\n",
    "    return model_fit, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # Dictionary to store the results\n",
    "\n",
    "# Iterate over each country\n",
    "for country in df_filteredall['geo\\\\TIME_PERIOD'].unique():\n",
    "    # Filter the data for the current country and reset the index\n",
    "    country_data = df_filteredall[df_filteredall['geo\\\\TIME_PERIOD'] == country]['Value'].reset_index(drop=True)\n",
    "\n",
    "    # Apply differencing to make the series stationary\n",
    "    country_data_diff = country_data.diff().dropna()\n",
    "\n",
    "    # Fit SARIMAX model and make predictions\n",
    "    model_fit, predictions = fit_sarimax_model(country_data_diff)\n",
    "\n",
    "    # Store the model fit and predictions for the country\n",
    "    results[country] = {'model_fit': model_fit, 'predictions': predictions}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Model 2\n",
    "\n",
    "Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_drop = df.dropna(subset=['Value'])\n",
    "df_drop = df_drop[(df_drop['indic'] == 'IS-EPI')]\n",
    "\n",
    "# Scale the 'Value' column\n",
    "df_drop['Value'] = StandardScaler().fit_transform(df_drop[['Value']])\n",
    "\n",
    "# select the relevant columns as features and target\n",
    "X = df_drop[['geo\\\\TIME_PERIOD', 's_adj']]\n",
    "y = df_drop['Value']\n",
    "\n",
    "# perform one-hot encoding on the categorical columns\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2)\n",
    "\n",
    "# create and fit the model\n",
    "model = SVR()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate the model's performance metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Support Vector Regression object\n",
    "svr = SVR()\n",
    "\n",
    "# define the hyperparameter grid to search over\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "\n",
    "# create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters found\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters to create a new model\n",
    "best_svr = SVR(C=100, gamma=0.1, kernel='linear')\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_svr.predict(X_test)\n",
    "\n",
    "# Calculate the model's performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the actual values and predicted values\n",
    "plt.scatter(y_test, y_pred, color='blue')\n",
    "\n",
    "# Add a diagonal line for comparison\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('SVR Predictions vs. Actual Values')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dashboard to show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')].copy()\n",
    "\n",
    "# Filter the DataFrame so only quarters from 2002 or later are included \n",
    "df_filtered = df_filtered[df_filtered['Quarter'] >= '2002-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Use interpolation to fill in missing values\n",
    "df_filtered['Value'] = df_filtered['Value'].interpolate(limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column to df_filtered called Value_Pred, with is the Value column + 10 - This will later be the predicted time series values.\n",
    "df_filtered[\"Value_Pred\"] = df_filtered[\"Value\"] + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BEL' 'BGR' 'CYP' nan 'EST' 'ESP' 'FIN' 'HRV' 'IRL' 'ITA' 'LTU' 'LVA'\n",
      " 'MNE' 'MKD' 'MLT' 'NLD' 'NOR' 'PRT' 'SWE' 'TUR']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of the 'Country_Codes' column\n",
    "print(df_filtered['Country_Codes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BE' 'BG' 'CY' 'EA19' 'EA20' 'EE' 'ES' 'FI' 'HR' 'IE' 'IT' 'LT' 'LV' 'ME'\n",
      " 'MK' 'MT' 'NL' 'NO' 'PT' 'SE' 'TR']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of the 'geo\\\\TIME_PERIOD' column\n",
    "print(df_filtered['geo\\\\TIME_PERIOD'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Where the 'Country_Codes' column is null, fill in the value of the 'geo\\\\TIME_PERIOD' column\n",
    "df_filtered['Country_Codes'] = df_filtered['Country_Codes'].fillna(df_filtered['geo\\\\TIME_PERIOD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to parse the date string\n",
    "def parse_quarter(date_string):\n",
    "    year, quarter = date_string.split('-Q')\n",
    "    month = (int(quarter) - 1) * 3 + 1\n",
    "    return datetime(int(year), month, 1)\n",
    "\n",
    "#Apply the parse_quarter function to the 'Quarter' column\n",
    "df_filtered['Quarter'] = df_filtered['Quarter'].apply(parse_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map country codes to full names\n",
    "country_names = {\n",
    " \"NLD\": \"Netherlands\",\n",
    " \"LTU\": \"Lithuania\",\n",
    " \"BEL\": \"Belgium\",\n",
    " \"BGR\": \"Bulgaria\",\n",
    " \"CYP\": \"Cyprus\",\n",
    " \"EST\": \"Estonia\",\n",
    " \"ESP\": \"Spain\",\n",
    " \"FIN\": \"Finland\",\n",
    " \"HRV\": \"Croatia\",\n",
    " \"IRL\": \"Ireland\",\n",
    " \"ITA\": \"Italy\",\n",
    " \"LVA\": \"Latvia\",\n",
    " \"MNE\": \"Montenegro\",\n",
    " \"MLT\": \"Malta\",\n",
    " \"NOR\": \"Norway\",\n",
    " \"PRT\": \"Portugal\",\n",
    " \"SWE\": \"Sweden\",\n",
    " \"TUR\": \"Turkey\",\n",
    " \"MKD\": \"North Macedonia\",\n",
    " \"EA19\" : \"Euro Area (19 countries, 2015-2022)\",\n",
    " \"EA20\" : \"Euro Area (20 countries from 2023)\",\n",
    "}\n",
    "\n",
    "# Create a new column in df_filtered with full country names\n",
    "df_filtered[\"Country_Names\"] = df_filtered[\"Country_Codes\"].map(country_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the EA19 and EA20 rows\n",
    "df_filtered = df_filtered[df_filtered[\"geo\\\\TIME_PERIOD\"].isin([\"EA19\", \"EA20\"]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo\\TIME_PERIOD</th>\n",
       "      <th>indic</th>\n",
       "      <th>s_adj</th>\n",
       "      <th>freq</th>\n",
       "      <th>unit</th>\n",
       "      <th>nace_r2</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Value</th>\n",
       "      <th>Country_Codes</th>\n",
       "      <th>pvalue</th>\n",
       "      <th>normal</th>\n",
       "      <th>Value_Pred</th>\n",
       "      <th>Country_Names</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36121</th>\n",
       "      <td>BE</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>97.600000</td>\n",
       "      <td>BEL</td>\n",
       "      <td>3.933126e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>107.600000</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36122</th>\n",
       "      <td>BG</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>66.900000</td>\n",
       "      <td>BGR</td>\n",
       "      <td>1.247375e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>76.900000</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36124</th>\n",
       "      <td>CY</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>140.300000</td>\n",
       "      <td>CYP</td>\n",
       "      <td>8.788795e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>150.300000</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36130</th>\n",
       "      <td>EE</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>68.900000</td>\n",
       "      <td>EST</td>\n",
       "      <td>6.030875e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>78.900000</td>\n",
       "      <td>Estonia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-01-01 - 68.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36132</th>\n",
       "      <td>ES</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>138.600000</td>\n",
       "      <td>ESP</td>\n",
       "      <td>4.409479e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>148.600000</td>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70586</th>\n",
       "      <td>NL</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>107.736364</td>\n",
       "      <td>NLD</td>\n",
       "      <td>9.571093e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>117.736364</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70587</th>\n",
       "      <td>NO</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>108.218182</td>\n",
       "      <td>NOR</td>\n",
       "      <td>3.948446e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>118.218182</td>\n",
       "      <td>Norway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70589</th>\n",
       "      <td>PT</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>108.700000</td>\n",
       "      <td>PRT</td>\n",
       "      <td>5.634062e-103</td>\n",
       "      <td>False</td>\n",
       "      <td>118.700000</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70592</th>\n",
       "      <td>SE</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>108.700000</td>\n",
       "      <td>SWE</td>\n",
       "      <td>2.019061e-03</td>\n",
       "      <td>False</td>\n",
       "      <td>118.700000</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70595</th>\n",
       "      <td>TR</td>\n",
       "      <td>IS-HWI</td>\n",
       "      <td>NSA</td>\n",
       "      <td>Q</td>\n",
       "      <td>I2015</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>108.700000</td>\n",
       "      <td>TUR</td>\n",
       "      <td>2.136857e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>118.700000</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1615 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      geo\\TIME_PERIOD   indic s_adj freq   unit nace_r2    Quarter  \\\n",
       "36121              BE  IS-HWI   NSA    Q  I2015       F 2002-01-01   \n",
       "36122              BG  IS-HWI   NSA    Q  I2015       F 2002-01-01   \n",
       "36124              CY  IS-HWI   NSA    Q  I2015       F 2002-01-01   \n",
       "36130              EE  IS-HWI   NSA    Q  I2015       F 2002-01-01   \n",
       "36132              ES  IS-HWI   NSA    Q  I2015       F 2002-01-01   \n",
       "...               ...     ...   ...  ...    ...     ...        ...   \n",
       "70586              NL  IS-HWI   NSA    Q  I2015       F 2023-01-01   \n",
       "70587              NO  IS-HWI   NSA    Q  I2015       F 2023-01-01   \n",
       "70589              PT  IS-HWI   NSA    Q  I2015       F 2023-01-01   \n",
       "70592              SE  IS-HWI   NSA    Q  I2015       F 2023-01-01   \n",
       "70595              TR  IS-HWI   NSA    Q  I2015       F 2023-01-01   \n",
       "\n",
       "            Value Country_Codes         pvalue  normal  Value_Pred  \\\n",
       "36121   97.600000           BEL   3.933126e-02   False  107.600000   \n",
       "36122   66.900000           BGR   1.247375e-07   False   76.900000   \n",
       "36124  140.300000           CYP   8.788795e-03   False  150.300000   \n",
       "36130   68.900000           EST   6.030875e-03   False   78.900000   \n",
       "36132  138.600000           ESP   4.409479e-03   False  148.600000   \n",
       "...           ...           ...            ...     ...         ...   \n",
       "70586  107.736364           NLD   9.571093e-07   False  117.736364   \n",
       "70587  108.218182           NOR   3.948446e-03   False  118.218182   \n",
       "70589  108.700000           PRT  5.634062e-103   False  118.700000   \n",
       "70592  108.700000           SWE   2.019061e-03   False  118.700000   \n",
       "70595  108.700000           TUR   2.136857e-02   False  118.700000   \n",
       "\n",
       "      Country_Names  Max                Min  \n",
       "36121       Belgium  NaN                NaN  \n",
       "36122      Bulgaria  NaN                NaN  \n",
       "36124        Cyprus  NaN                NaN  \n",
       "36130       Estonia  NaN  2002-01-01 - 68.9  \n",
       "36132         Spain  NaN                NaN  \n",
       "...             ...  ...                ...  \n",
       "70586   Netherlands  NaN                NaN  \n",
       "70587        Norway  NaN                NaN  \n",
       "70589      Portugal  NaN                NaN  \n",
       "70592        Sweden  NaN                NaN  \n",
       "70595        Turkey  NaN                NaN  \n",
       "\n",
       "[1615 rows x 15 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_max_column(df_filtered):\n",
    "    max_values = df_filtered.groupby(\"Country_Codes\")[\"Value\"].transform(\"max\")\n",
    "    max_indices = df_filtered[\"Value\"] == max_values\n",
    "    df_filtered[\"Max\"] = df_filtered.loc[max_indices, \"Quarter\"].dt.strftime(\"%Y-%m-%d\") + \" - \" + df_filtered.loc[max_indices, \"Value\"].astype(str)\n",
    "    return df_filtered\n",
    "\n",
    "add_max_column(df_filtered)\n",
    "\n",
    "def add_min_column(df_filtered):\n",
    "    min_values = df_filtered.groupby(\"Country_Codes\")[\"Value\"].transform(\"min\")\n",
    "    min_indices = df_filtered[\"Value\"] == min_values\n",
    "    df_filtered[\"Min\"] = df_filtered.loc[min_indices, \"Quarter\"].dt.strftime(\"%Y-%m-%d\") + \" - \" + df_filtered.loc[min_indices, \"Value\"].astype(str)\n",
    "    return df_filtered\n",
    "\n",
    "add_min_column(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# filter out all rows where the 'Max' column is null and assign the result to df_max\n",
    "df_max = df_filtered[df_filtered['Max'].notnull()]\n",
    "\n",
    "# filter out all rows where the 'Min' column is null and assign the result to df_min\n",
    "df_min = df_filtered[df_filtered['Min'].notnull()]\n",
    "\n",
    "# Merge df_filtered and df_max on the 'Country_Codes' column using a left join, keep only the Max column from df_max and rename it to 'Max'\n",
    "df_filtered = df_filtered.merge(df_max[['Country_Codes', 'Max']], on='Country_Codes', how='left').rename(columns={'Max': 'Max'})\n",
    "\n",
    "# Merge df_filtered and df_min on the 'Country_Codes' column using a left join, keep only the Min column from df_min and rename it to 'Min'\n",
    "df_filtered = df_filtered.merge(df_min[['Country_Codes', 'Min']], on='Country_Codes', how='left').rename(columns={'Min': 'Min'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   geo\\TIME_PERIOD   Latitude   Longitude\n",
      "0               BE  50.640281    4.666715\n",
      "1               BG  42.607397   25.485662\n",
      "2               CY  34.982302   33.145128\n",
      "3               EE  58.752378   25.331908\n",
      "4               ES  39.326068   -4.837979\n",
      "5               FI  63.246778   25.920916\n",
      "6               HR  45.365844   15.657521\n",
      "7               IE  52.865196   -7.979460\n",
      "8               IT  42.638426   12.674297\n",
      "9               LT  55.350000   23.750000\n",
      "10              LV  56.840649   24.753764\n",
      "11              ME  23.658512 -102.007710\n",
      "12              MK  41.617121   21.716839\n",
      "13              MT  35.888599   14.447691\n",
      "14              NL  52.247650    5.541247\n",
      "15              NO  61.152939    8.787665\n",
      "16              PT  39.662165   -8.135352\n",
      "17              SE  59.674971   14.520858\n",
      "18              TR  38.959759   34.924965\n"
     ]
    }
   ],
   "source": [
    "#Create dashboard layout\n",
    "# Create initial figures\n",
    "fig_line = px.line(df_filtered, x=\"Quarter\", y=\"Value\", color=\"Country_Codes\") \n",
    "fig_map = px.choropleth(df_filtered, locations=\"Country_Codes\", color=\"Value\") \n",
    "\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"my-app\")  # Initialize the geolocator\n",
    "\n",
    "# country_codes is equal to the unique values of the 'geo\\\\TIME_PERIOD' column, filtering out EA19 and EA20\n",
    "country_codes = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] != 'EA19'][df_filtered['geo\\\\TIME_PERIOD'] != 'EA20']['geo\\\\TIME_PERIOD'].unique()\n",
    "\n",
    "data = {\"geo\\\\TIME_PERIOD\": country_codes, \"Latitude\": [], \"Longitude\": []}\n",
    "\n",
    "for country_code in country_codes:\n",
    "    location = geolocator.geocode(country_code)\n",
    "    if location is not None:\n",
    "        data[\"Latitude\"].append(location.latitude)\n",
    "        data[\"Longitude\"].append(location.longitude)\n",
    "    else:\n",
    "        data[\"Latitude\"].append(None)\n",
    "        data[\"Longitude\"].append(None)\n",
    "\n",
    "df_coords = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Merge df_filtered and df_coords on the 'geo\\\\TIME_PERIOD' column using a left join where df_filtered is on the left, keep only the 'Latitude' and 'Longitude' columns from df_coords and rename them to 'Latitude' and 'Longitude'\n",
    "df_filtered = df_filtered.merge(df_coords[['geo\\\\TIME_PERIOD', 'Latitude', 'Longitude']], left_on='geo\\\\TIME_PERIOD', right_on='geo\\\\TIME_PERIOD', how='left').rename(columns={'Latitude': 'Latitude', 'Longitude': 'Longitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "# Add another line to fig_line based on Value_Pred column\n",
    "fig_line.add_trace(go.Scatter(x=df_filtered[\"Quarter\"], y=df_filtered[\"Value_Pred\"], name=\"Predicted Value\", mode=\"lines\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "\n",
    "# Create app layout\n",
    "app = JupyterDash(__name__) # Use JupyterDash instead of Dash\n",
    "app.layout = html.Div([\n",
    " html.H2(\n",
    " \"Quarterly Index (Non-Seasonally Adjusted)\", \n",
    " id=\"heading\",\n",
    " style={\"textAlign\": \"center\"}\n",
    " ),\n",
    " dcc.RadioItems( \n",
    " id=\"country-buttons\", \n",
    " options=[{\"label\": c, \"value\": c} for c in df_filtered[\"Country_Names\"].unique()], # Use full country names as labels\n",
    " value=df_filtered[\"Country_Names\"].iloc[0],\n",
    " style={\"table-layout\": \"fixed\", \"width\": \"50%\"}, # Style the container element \n",
    " labelStyle={\"display\": \"inline-block\", \"margin-right\": \"10px\"} # Style each label element\n",
    " ),\n",
    " html.Div([ # Wrap line chart and map in a div with flex display\n",
    "     html.Div([ # Wrap dropdown in a div with 50% width\n",
    "         dcc.Graph(id=\"map-chart\", figure=fig_map)\n",
    "     ], style={\"width\": \"50%\"}),\n",
    "     html.Div([ # Wrap map in a div with 50% width\n",
    "         dcc.Graph(id=\"line-chart\", figure=fig_line)\n",
    "     ], style={\"width\": \"100%\"})\n",
    " ], style={\"display\": \"flex\", \"flex-direction\": \"row\"}),\n",
    "])\n",
    "\n",
    "# Define callback function\n",
    "@app.callback(\n",
    " [Output(\"line-chart\", \"figure\"), Output(\"map-chart\", \"figure\"), Output(\"heading\", \"children\")],\n",
    " [Input(\"country-buttons\", \"value\")]\n",
    ")\n",
    "def update_charts(country_name):\n",
    " # Filter dataframe by selected country name\n",
    " df_country = df_filtered[df_filtered[\"Country_Names\"] == country_name] \n",
    " # Create new figures\n",
    " fig_line = px.line(df_country, x=\"Quarter\", y=\"Value\", color=\"Country_Codes\", hover_data={\"Country_Names\": True, \"Quarter\": True,\"Value\": True, \"Country_Codes\": False}, labels={\"Country_Names\": \"Country\", \"Quarter\": \"Quarter\", \"Value\": \"Value\"}) \n",
    " fig_map = px.choropleth(df_country, locations=\"Country_Codes\", color=\"Value\", hover_data={\"Country_Names\": True, \"Max_y\": True,\"Min_y\": True},labels={\"Country_Names\": \"Country\", \"Max_y\": \"Max\", \"Min_y\": \"Min\"}) \n",
    " # Add another line to fig_line based on Value_Pred column\n",
    " fig_line.add_trace(go.Scatter(x=df_country[\"Quarter\"], y=df_country[\"Value_Pred\"], name=\"Predicted Value\", mode=\"lines\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "  # Change the Legend Title\n",
    " fig_line.update_layout(\n",
    "    legend=dict(\n",
    "    title=dict(text=\"Value Type\"),\n",
    "    itemclick=\"toggle\",\n",
    "    itemdoubleclick=\"toggleothers\",\n",
    "    )\n",
    " )\n",
    " # Update the hovermode and showlegend\n",
    " fig_map.update_layout(\n",
    "    hovermode='closest'\n",
    " )\n",
    " \n",
    " #Customise Map Tooltip Options\n",
    " fig_map.update_traces(hovertemplate=\"<b>%{customdata[0]}</b><br>Max: %{customdata[1]}<br>Min: %{customdata[2]}<extra></extra>\")\n",
    " \n",
    " # Use custom values for center based on latitude and longitude columns and higher value for projection.scale \n",
    " fig_map.update_layout(geo=dict(center=dict(lat=df_country['Latitude'].iloc[0], lon=df_country['Longitude'].iloc[0]), projection_scale=5))\n",
    "\n",
    " \n",
    " # Return new figures\n",
    " return fig_line, fig_map, f\"Quarterly Index (Non-Seasonally Adjusted) for {country_name}\"\n",
    "\n",
    "\n",
    "# Run app\n",
    "app.run_server(mode=\"external\") # Set mode to \"inline\" or \"external\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
