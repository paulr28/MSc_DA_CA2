{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import praw\n",
    "from textblob import TextBlob\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import wilcoxon\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "from jupyter_dash import JupyterDash\n",
    "from geopy.geocoders import Nominatim\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from numpy import log\n",
    "import itertools\n",
    "from itertools import product\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/EI_ISBU_Q?format=TSV&compressed=true' # The url of the data\n",
    "headers = {'Accept-Encoding': 'gzip'} # This is important to get the gzip file\n",
    "response = requests.get(url, headers=headers) # Get the data from the url\n",
    "\n",
    "buf = BytesIO(response.content) # Read the gzip file\n",
    "f = gzip.GzipFile(fileobj=buf) # Unzip the gzip file\n",
    "content = f.read() # Read the unzipped file\n",
    "\n",
    "df = pd.read_csv(BytesIO(content), sep='\\t') # Read the unzipped file as a dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Melt the dataframe so all the Quarters are in one column\n",
    "df = pd.melt(df, id_vars=['freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD'], var_name='Quarter', value_name='Value')\n",
    "\n",
    "# The column freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD is split into 6 columns\n",
    "df[['freq','unit','s_adj','indic','nace_r2','geo\\TIME_PERIOD']] = df['freq,unit,s_adj,indic,nace_r2,geo\\\\TIME_PERIOD'].str.split(',', expand=True)\n",
    "\n",
    "# Drop the column freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD and move Quarter and Value to the back\n",
    "df = df.drop(['freq,unit,s_adj,indic,nace_r2,geo\\\\TIME_PERIOD'], axis=1)\n",
    "df = df[['freq', 'unit', 's_adj', 'indic', 'nace_r2','geo\\TIME_PERIOD','Quarter', 'Value']]\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# set values containing \":\" to NaN\n",
    "df.loc[df['Value'].str.contains(':'), 'Value'] = pd.np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# if the Value column has an alphabetical character remove it\n",
    "df['Value'] = df['Value'].str.replace('[a-zA-Z]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Convert the Value column to a float\n",
    "df['Value'] = df['Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "countrycode_map = {c.alpha_2: c.alpha_3 for c in pycountry.countries} # Create a dictionary of country codes\n",
    "\n",
    "#Uk is not a country code so we will change it to GB\n",
    "df.loc[df['geo\\\\TIME_PERIOD'] == 'UK', 'geo\\\\TIME_PERIOD'] = 'GB'\n",
    "\n",
    "df['Country_Codes'] = df['geo\\\\TIME_PERIOD'].map(countrycode_map) # Map the country codes to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Print unique values of the Country_Codes column\n",
    "print(df['Country_Codes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Perform Normailty tests for the Value column grouped by geo\\TIME_PERIOD, indic, and s_adj\n",
    "\n",
    "df.set_index(['geo\\\\TIME_PERIOD', 'indic', 's_adj'], inplace=True)\n",
    "\n",
    "# Perform normality test on the Value column grouped by geo\\TIME_PERIOD, indic, and s_adj\n",
    "grouped = df.groupby(['geo\\\\TIME_PERIOD', 'indic', 's_adj'])['Value']\n",
    "pvalues = grouped.apply(lambda x: normaltest(x.dropna())[1])\n",
    "\n",
    "# Add p-values to a new column in the original DataFrame\n",
    "df.loc[pvalues.index, 'pvalue'] = pvalues\n",
    "\n",
    "#Add a new column to the dfi dataframe called 'normal' and set it to True if the pvalue is greater than 0.05 and False if it is less than 0.05\n",
    "df['normal'] = df['pvalue'] > 0.05\n",
    "\n",
    "# Move the index back to columns\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a country plot with the Value column as the color and the Quarter column as the animation, show for indicators '[IS-IP]' and Seasonal Adjustment 'NSA'\n",
    "\n",
    "fig = px.choropleth(df[(df['indic'] == 'IS-IP') & (df['s_adj'] == 'NSA')], locations=\"Country_Codes\",color=\"Value\", hover_name=\"geo\\TIME_PERIOD\", animation_frame=\"Quarter\", color_continuous_scale=px.colors.sequential.Plasma, range_color=(0, 100), scope='europe') # Create the plot\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by geo\\TIME_PERIOD, indic, and s_adj and show descriptive statistics\n",
    "df.groupby(['s_adj', 'indic', 'geo\\\\TIME_PERIOD']).describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the geo\\TIME_PERIOD value that equals 'IE' into a seperate dataframe\n",
    "IE = df[df['geo\\TIME_PERIOD'] == 'IE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform EDA on the IE dataframe\n",
    "IE.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Statistical Analysis on the IE dataframe\n",
    "IE.describe(include='all')\n",
    "\n",
    "IE.groupby(['indic','s_adj']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histomgrams of the Value column for the IE dataframe grouped by 'indic' and 's_adj', faceted by the 'normal' column\n",
    "sns.FacetGrid(IE, col='indic', row='s_adj', hue='normal').map(sns.histplot, 'Value').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms of the Value column for the IE dataframe grouped by 'indic' and 's_adj', where the 'normal' column is True\n",
    "sns.FacetGrid(IE[IE['normal'] == True], row='indic', col='s_adj', hue= 's_adj').map(sns.histplot, 'Value').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the IE dataframe showing statistical analysis using seaborn  and matplotlib  libraries splitting the data by insdicators and seasonanal adjustment\n",
    "sns.catplot(x=\"indic\", y=\"Value\", hue=\"s_adj\", kind=\"box\", data=IE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the unique geo\\TIME_PERIOD values from the df dataframe, where the indic column equals 'IS-EPI' and the normal column equals True\n",
    "df[(df['indic'] == 'IS-EPI') & (df['normal'] == True)]['geo\\\\TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Ireland with 'EU28' which is the overall EU average for 28 countries\n",
    "\n",
    "# Create a dataframe called IE_eu28 that contains the geo\\TIME_PERIOD values of 'IE' and 'EU28'\n",
    "IE_eu28 = df[(df['geo\\\\TIME_PERIOD'] == 'IE') | (df['geo\\\\TIME_PERIOD'] == 'EU28')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Number of persons employed index, non-seasonally adjusted, for Ireland and EU28 using a t-test\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Filter the DataFrame to only include rows with geo\\TIME_PERIOD values of 'EU28' and 'IE',\n",
    "# 'IS-EPI' for indic, and 'NSA' for s_adj\n",
    "df_split = df[(df['geo\\\\TIME_PERIOD'].isin(['EU28', 'IE'])) & (df['indic'] == 'IS-EPI') & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Split the DataFrame into two separate DataFrames, one for each geo\\TIME_PERIOD value\n",
    "df_eu281 = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == 'EU28']\n",
    "IE1 = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == 'IE']\n",
    "\n",
    "# Perform the t-test\n",
    "t_stat, p_val = ttest_ind(df_eu281['Value'], IE1['Value'], equal_var=False)\n",
    "\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a One-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include rows where the normal column equals True and the indic column equals 'IS-EPI' and the s_adj column equals 'NSA'\n",
    "df_filtered = df[(df['normal'] == True) & (df['indic'] == 'IS-EPI') & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Create a list of the unique geo\\TIME_PERIOD values\n",
    "geo_time_periods = df_filtered['geo\\\\TIME_PERIOD'].unique()\n",
    "\n",
    "#Make sure the groups have the same sample size\n",
    "min_sample_size = df_filtered['geo\\\\TIME_PERIOD'].value_counts().min()\n",
    "df_filtered = df_filtered.groupby('geo\\\\TIME_PERIOD').apply(lambda x: x.sample(min_sample_size))\n",
    "\n",
    "#Perform the ANOVA\n",
    "f_stat, p_val = f_oneway(*[df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == geo_time_period]['Value'] for geo_time_period in geo_time_periods])\n",
    "\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "#Print th resuts of the ANOVA test\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject null hypothesis - Significant differences exist between groups.\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - No significant difference between groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the diferences between the groups using a boxplot with slanted x-axis labels\n",
    "sns.catplot(x=\"geo\\\\TIME_PERIOD\", y=\"Value\", kind=\"box\", data=df_filtered).set_xticklabels(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Tukey's Range Test to determine which groups are significantly different from each other\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's Range Test\n",
    "tukey_results = pairwise_tukeyhsd(df_filtered['Value'], df_filtered['geo\\\\TIME_PERIOD'], 0.05)\n",
    "\n",
    "# Print the results\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Two-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe called prep1 where the indic column equals 'IS-EPI' and the s_adj column contains 'SCA and 'NSA' values and the normal column equals True\n",
    "prep1 = df[(df['indic'] == 'IS-EPI') & (df['normal'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the name of the geo\\TIME_PERIOD column to geo in the df dataframe\n",
    "prep1.rename(columns={'geo\\\\TIME_PERIOD': 'geo'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a TWO-WAY ANOVA to determine if there is an interaction between the geo\\TIME_PERIOD and s_adj columns\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Value ~ s_adj + geo', data = prep1).fit()\n",
    "aov2 = sm.stats.anova_lm(model, type=2)\n",
    "print(aov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Wilcoxon Signed-Rank Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find countries that have false values for the normal column where the indic column equals 'IS-HWI'\n",
    "df[(df['normal'] == False) & (df['indic'] == 'IS-HWI')]['geo\\\\TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_HWI = df[(df['indic'] == 'IS-HWI')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_HWI = df_HWI.dropna(subset=['Value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the two groups to compare\n",
    "group1 = df_HWI[df_HWI['geo\\\\TIME_PERIOD'] == 'IE']['Value']\n",
    "group2 = df_HWI[df_HWI['geo\\\\TIME_PERIOD'] == 'EU28']['Value']\n",
    "\n",
    "# Ensure the two groups have the same sample size\n",
    "min_sample_size = min(len(group1), len(group2))\n",
    "group1 = group1.sample(min_sample_size)\n",
    "group2 = group2.sample(min_sample_size)\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p = wilcoxon(group1, group2)\n",
    "\n",
    "# Print the results\n",
    "print('Wilcoxon signed-rank test:')\n",
    "print(f'statistic: {stat:.4f}')\n",
    "print(f'p-value: {p:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Kruskall Wallis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "stat, p = kruskal(*[df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == geo]['Value'] for geo in df_filtered['geo\\\\TIME_PERIOD'].unique()])\n",
    "\n",
    "# Print the results\n",
    "print(\"Kruskal-Wallis Test Results:\")\n",
    "print(f\"Test statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the environmental variables\n",
    "APP_NAME = getenv('APP_NAME')\n",
    "APP_ID = getenv(\"APP_ID\")\n",
    "APP_SECRET = getenv(\"APP_SECRET\")\n",
    "USERNAME = getenv('REDDIT_USERNAME')\n",
    "PASSWORD = getenv('PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=APP_ID,\n",
    "    client_secret=APP_SECRET,\n",
    "    user_agent=APP_NAME,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    ")\n",
    "\n",
    "# Define the subreddits and search query\n",
    "subreddits = [\"Ireland\", \"Europe\"]\n",
    "query = \"house prices\"\n",
    "\n",
    "# Collect posts from the subreddits related to the search query\n",
    "posts = []\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for post in subreddit.search(query):\n",
    "        posts.append(\n",
    "            {\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"title\": post.title,\n",
    "                \"text\": post.selftext,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the collected posts into a dataframe\n",
    "df_posts = pd.DataFrame(posts)\n",
    "\n",
    "# Perform sentiment analysis on the collected posts\n",
    "df_posts[\"polarity\"] = df_posts[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Group the results by subreddit and calculate the mean polarity\n",
    "results = df_posts.groupby(\"subreddit\")[\"polarity\"].mean()\n",
    "\n",
    "# Print the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the results of the sentiment analysis\n",
    "if results[\"Ireland\"] > results[\"Europe\"]:\n",
    "    print(\"The sentiment of the posts from r/Ireland is more positive than the sentiment of the posts from r/Europe.\")\n",
    "elif results[\"Ireland\"] < results[\"Europe\"]:\n",
    "    print(\"The sentiment of the posts from r/Ireland is more negative than the sentiment of the posts from r/Europe.\")\n",
    "else:\n",
    "    print(\"The sentiment of the posts from r/Ireland is the same as the sentiment of the posts from r/Europe.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Analysis 1.1\n",
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "df_split = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')].copy()\n",
    "\n",
    "#Create a df named df_split that contains only the Quarter, Value and geo\\TIME_PERIOD columns\n",
    "df_split = df_split[['Quarter', 'Value', 'geo\\\\TIME_PERIOD']]\n",
    "\n",
    "# Filter the DataFrame so only quarters from 2002 or later are included \n",
    "df_split = df_split[df_split['Quarter'] >= '2002-01-01']\n",
    "\n",
    "df_split = df_split[df_split['Quarter'] < '2023-01-01']\n",
    "\n",
    "# Use interpolation to fill in missing values\n",
    "df_split['Value'] = df_split['Value'].interpolate(limit_direction='both')\n",
    "\n",
    "# Define a function to parse the date string\n",
    "def parse_quarter(date_string):\n",
    "    year, quarter = date_string.split('-Q')\n",
    "    month = (int(quarter) - 1) * 3 + 1\n",
    "    return datetime(int(year), month, 1)\n",
    "\n",
    "#Apply the parse_quarter function to the 'Quarter' column\n",
    "df_split['Quarter'] = df_split['Quarter'].apply(parse_quarter)\n",
    "\n",
    "# Set the 'Quarter' column as the index\n",
    "df_split.set_index('Quarter', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the unique values in the geo\\TIME_PERIOD column\n",
    "geo_list = df_split['geo\\\\TIME_PERIOD'].unique()\n",
    "\n",
    "# Print unique values of the geo\\TIME_PERIOD column\n",
    "print(df_split['geo\\\\TIME_PERIOD'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into separate countries\n",
    "for country in df_split['geo\\\\TIME_PERIOD'].unique():\n",
    "    exec('{} = df_split[df_split[\"geo\\\\TIME_PERIOD\"] == country][\"Value\"]'.format(country))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Analysis 1.2\n",
    "Ireland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot of the 'Value' column\n",
    "IE.plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Hours Worked Index in Ireland')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE = IE.asfreq('QS-OCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert series to dataframe\n",
    "df_IE = IE.to_frame()\n",
    "\n",
    "# Reset the index of df_IE\n",
    "df_IE.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Year and Month columns\n",
    "df_IE['year'] = [d.year for d in df_IE.Quarter]\n",
    "df_IE['month'] = [d.strftime('%b') for d in df_IE.Quarter]\n",
    "years = df_IE['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)\n",
    "sns.boxplot(x='year', y='Value', data=df_IE, ax=axes[0])\n",
    "sns.boxplot(x='month', y='Value', data=df_IE)\n",
    "\n",
    "# Set Title\n",
    "axes[0].set_title('Year-wise Box Plot\\n(The Trend)', fontsize=18); \n",
    "axes[1].set_title('Month-wise Box Plot\\n(The Seasonality)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(IE, model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(IE, model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)\n",
    "result_add.plot().suptitle('Additive Decompose', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out if the data is stationary or non-stationary\n",
    "adf_test = adfuller(IE)\n",
    "print(f'ADF Statistic: {adf_test[0]}')\n",
    "print(f'p-value: {adf_test[1]}')\n",
    "print(f'Critical Values: {adf_test[4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Original Series\n",
    "fig, axes = plt.subplots(3, 2, sharex=True)\n",
    "axes[0, 0].plot(IE); axes[0, 0].set_title('Original Series')\n",
    "plot_acf(IE, ax=axes[0, 1])\n",
    "\n",
    "# 1st Differencing\n",
    "axes[1, 0].plot(IE.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(IE.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "# 2nd Differencing\n",
    "axes[2, 0].plot(IE.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(IE.diff().diff().dropna(), ax=axes[2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10,5), dpi=100, sharex=True)\n",
    "\n",
    "# Usual Differencing\n",
    "axes[0].plot(IE[:], label='Original Series')\n",
    "axes[0].plot(IE[:].diff(1), label='Usual Differencing')\n",
    "axes[0].set_title('Usual Differencing')\n",
    "axes[0].legend(loc='upper left', fontsize=10)\n",
    "\n",
    "\n",
    "# Seasinal Dei\n",
    "axes[1].plot(IE[:], label='Original Series')\n",
    "axes[1].plot(IE[:].diff(12), label='Seasonal Differencing', color='green')\n",
    "axes[1].set_title('Seasonal Differencing')\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.suptitle('Hours Worked Index - Ireland', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test sets\n",
    "train = IE[:len(IE)-12]\n",
    "test = IE[len(IE)-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create a SARIMAX model\n",
    "model = sm.tsa.statespace.SARIMAX(train)\n",
    "fit_model = model.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = fit_model.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions.plot(figsize=(12, 5))\n",
    "\n",
    "# Plot the actual values\n",
    "IE[len(IE)-12:].plot()\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions vs Actual for the last 2 Years')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(['Predictions', 'Actual'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test, predictions))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test, predictions, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the models hyperparameters to improve the RMSE\n",
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "# Find the optimal set of parameters that yields the best performance\n",
    "# Define the initial parameters\n",
    "best_score, best_params, best_seasonal_params = float(\"inf\"), None, None\n",
    "\n",
    "# Loop through the parameter combinations\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            # Create a SARIMAX model\n",
    "            model = sm.tsa.statespace.SARIMAX(train, order=param, seasonal_order=param_seasonal)\n",
    "\n",
    "            # Fit the model\n",
    "            results = model.fit()\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = results.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "            # Calculate the mean squared error\n",
    "            mse = mean_squared_error(test, predictions)\n",
    "\n",
    "            # If the mse is lower than our best score, update the best score, and best parameters\n",
    "            if mse < best_score:\n",
    "                best_score, best_params, best_seasonal_params = mse, param, param_seasonal\n",
    "\n",
    "            # Print the model parameters and the mean squared error\n",
    "            print('SARIMA{}x{}4 - AIC:{}'.format(param, param_seasonal, mse))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "# Print the best model parameters and the mean squared error\n",
    "print('Best SARIMA{}x{}4 AIC:{}'.format(best_params, best_seasonal_params, best_score))\n",
    "\n",
    "# Turn warnings back on\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tuned model\n",
    "model1 = sm.tsa.statespace.SARIMAX(train,order=(0, 1, 0), seasonal_order=(0, 1, 0, 4))\n",
    "fit_model1 = model1.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions1 = fit_model1.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions1)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions1.plot(figsize=(12, 5))\n",
    "\n",
    "# Plot the actual values\n",
    "IE[len(IE)-12:].plot()\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions vs Actual for the last 2 Years')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(['Predictions', 'Actual'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test, predictions1))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test, predictions1, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fit_model1.plot_diagnostics(figsize=(12, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "pred = results.predict()\n",
    "\n",
    "# Plot the actual values and the predicted values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(df_filtered['Value'], label='Actual')\n",
    "ax.plot(pred, label='Predicted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction for the next 4 values\n",
    "pred = results.predict(start=len(IE), end=len(IE) + 3)\n",
    "\n",
    "# Print the predictions\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Current Year with actual and predicted values and the Next Year with predicted values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(IE[len(IE)-12:], label='Actual')\n",
    "ax.plot(pred, label='Predicted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the actual and predicted values\n",
    "compare = pd.DataFrame({'Actual': IE[len(IE)-12:], 'Predicted': pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs predicted values from the compare DataFrame\n",
    "compare.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Analysis 1.3 \n",
    "Across countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# For each geo_list item, set the freq to 'QS-OCT'\n",
    "for geo in geo_list:\n",
    "    globals()[geo].index.freq = 'QS-OCT'\n",
    "\n",
    "# List of dataframe names\n",
    "dataframe_names = geo_list\n",
    "\n",
    "# Create an empty list to store the comparison dataframes\n",
    "comparison_dataframes = []\n",
    "\n",
    "# Loop over the dataframe names\n",
    "for name in dataframe_names:\n",
    "    # Access the dataframe using the name\n",
    "    dataframe = globals()[name]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train = dataframe[:len(dataframe)-12]\n",
    "    test = dataframe[len(dataframe)-12:]\n",
    "    \n",
    "    # Create a model\n",
    "    model = sm.tsa.statespace.SARIMAX(train, order=(0, 1, 0), seasonal_order=(0, 1, 0, 4))\n",
    "    fit_model = model.fit()\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = fit_model.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "    \n",
    "    # Create a prediction for the next 4 values\n",
    "    pred = fit_model.predict(start=len(dataframe), end=len(dataframe) + 3)\n",
    "    \n",
    "    # Create a DataFrame of the actual and predicted values\n",
    "    compare = pd.DataFrame({'Actual': test, 'Predicted': pred})\n",
    "    \n",
    "    # Append the comparison dataframe to the list\n",
    "    comparison_dataframes.append(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the comparison dataframe and rename it to Quarter\n",
    "comparison_dataframes[0].reset_index(inplace=True)\n",
    "comparison_dataframes[0].rename(columns={'index': 'Quarter'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.concat([df.assign(geo=name) for name, df in zip(geo_list, comparison_dataframes)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the first 16 Quarter values and apply them to each geo group\n",
    "quarters = comp_df['Quarter'].unique()[:16]\n",
    "\n",
    "# Add quarters to each geo group\n",
    "comp_df['Quarter'] = np.tile(quarters, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataframme in the comparison_dataframes list, create a plot, with the title set to the dataframe name\n",
    "for df in comparison_dataframes:\n",
    "    df.plot(title=df.columns[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Model 2\n",
    "\n",
    "Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_drop = df.dropna(subset=['Value'])\n",
    "df_drop = df_drop[(df_drop['indic'] == 'IS-EPI')]\n",
    "\n",
    "# Scale the 'Value' column\n",
    "df_drop['Value'] = StandardScaler().fit_transform(df_drop[['Value']])\n",
    "\n",
    "# select the relevant columns as features and target\n",
    "X = df_drop[['geo\\\\TIME_PERIOD', 's_adj']]\n",
    "y = df_drop['Value']\n",
    "\n",
    "# perform one-hot encoding on the categorical columns\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2)\n",
    "\n",
    "# create and fit the model\n",
    "model = SVR()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate the model's performance metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Support Vector Regression object\n",
    "svr = SVR()\n",
    "\n",
    "# define the hyperparameter grid to search over\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "\n",
    "# create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters found\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters to create a new model\n",
    "best_svr = SVR(C=100, gamma=0.1, kernel='linear')\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_svr.predict(X_test)\n",
    "\n",
    "# Calculate the model's performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the actual values and predicted values\n",
    "plt.scatter(y_test, y_pred, color='blue')\n",
    "\n",
    "# Add a diagonal line for comparison\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('SVR Predictions vs. Actual Values')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dashboard to show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')].copy()\n",
    "\n",
    "# Filter the DataFrame so only quarters from 2002 or later are included \n",
    "df_filtered = df_filtered[df_filtered['Quarter'] >= '2002-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Use interpolation to fill in missing values\n",
    "df_filtered['Value'] = df_filtered['Value'].interpolate(limit_direction='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column to df_filtered called Value_Pred, with is the Value column + 10 - This will later be the predicted time series values.\n",
    "df_filtered[\"Value_Pred\"] = df_filtered[\"Value\"] + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Print unique values of the 'Country_Codes' column\n",
    "print(df_filtered['Country_Codes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of the 'geo\\\\TIME_PERIOD' column\n",
    "print(df_filtered['geo\\\\TIME_PERIOD'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Where the 'Country_Codes' column is null, fill in the value of the 'geo\\\\TIME_PERIOD' column\n",
    "df_filtered['Country_Codes'] = df_filtered['Country_Codes'].fillna(df_filtered['geo\\\\TIME_PERIOD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to parse the date string\n",
    "def parse_quarter(date_string):\n",
    "    year, quarter = date_string.split('-Q')\n",
    "    month = (int(quarter) - 1) * 3 + 1\n",
    "    return datetime(int(year), month, 1)\n",
    "\n",
    "#Apply the parse_quarter function to the 'Quarter' column\n",
    "df_filtered['Quarter'] = df_filtered['Quarter'].apply(parse_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map country codes to full names\n",
    "country_names = {\n",
    " \"NLD\": \"Netherlands\",\n",
    " \"LTU\": \"Lithuania\",\n",
    " \"BEL\": \"Belgium\",\n",
    " \"BGR\": \"Bulgaria\",\n",
    " \"CYP\": \"Cyprus\",\n",
    " \"EST\": \"Estonia\",\n",
    " \"ESP\": \"Spain\",\n",
    " \"FIN\": \"Finland\",\n",
    " \"HRV\": \"Croatia\",\n",
    " \"IRL\": \"Ireland\",\n",
    " \"ITA\": \"Italy\",\n",
    " \"LVA\": \"Latvia\",\n",
    " \"MNE\": \"Montenegro\",\n",
    " \"MLT\": \"Malta\",\n",
    " \"NOR\": \"Norway\",\n",
    " \"PRT\": \"Portugal\",\n",
    " \"SWE\": \"Sweden\",\n",
    " \"TUR\": \"Turkey\",\n",
    " \"MKD\": \"North Macedonia\",\n",
    " \"EA19\" : \"Euro Area (19 countries, 2015-2022)\",\n",
    " \"EA20\" : \"Euro Area (20 countries from 2023)\",\n",
    "}\n",
    "\n",
    "# Create a new column in df_filtered with full country names\n",
    "df_filtered[\"Country_Names\"] = df_filtered[\"Country_Codes\"].map(country_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the EA19 and EA20 rows\n",
    "df_filtered = df_filtered[df_filtered[\"geo\\\\TIME_PERIOD\"].isin([\"EA19\", \"EA20\"]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def add_max_column(df_filtered):\n",
    "    max_values = df_filtered.groupby(\"Country_Codes\")[\"Value\"].transform(\"max\")\n",
    "    max_indices = df_filtered[\"Value\"] == max_values\n",
    "    df_filtered[\"Max\"] = df_filtered.loc[max_indices, \"Quarter\"].dt.strftime(\"%Y-%m-%d\") + \" - \" + df_filtered.loc[max_indices, \"Value\"].astype(str)\n",
    "    return df_filtered\n",
    "\n",
    "add_max_column(df_filtered)\n",
    "\n",
    "def add_min_column(df_filtered):\n",
    "    min_values = df_filtered.groupby(\"Country_Codes\")[\"Value\"].transform(\"min\")\n",
    "    min_indices = df_filtered[\"Value\"] == min_values\n",
    "    df_filtered[\"Min\"] = df_filtered.loc[min_indices, \"Quarter\"].dt.strftime(\"%Y-%m-%d\") + \" - \" + df_filtered.loc[min_indices, \"Value\"].astype(str)\n",
    "    return df_filtered\n",
    "\n",
    "add_min_column(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# filter out all rows where the 'Max' column is null and assign the result to df_max\n",
    "df_max = df_filtered[df_filtered['Max'].notnull()]\n",
    "\n",
    "# filter out all rows where the 'Min' column is null and assign the result to df_min\n",
    "df_min = df_filtered[df_filtered['Min'].notnull()]\n",
    "\n",
    "# Merge df_filtered and df_max on the 'Country_Codes' column using a left join, keep only the Max column from df_max and rename it to 'Max'\n",
    "df_filtered = df_filtered.merge(df_max[['Country_Codes', 'Max']], on='Country_Codes', how='left').rename(columns={'Max': 'Max'})\n",
    "\n",
    "# Merge df_filtered and df_min on the 'Country_Codes' column using a left join, keep only the Min column from df_min and rename it to 'Min'\n",
    "df_filtered = df_filtered.merge(df_min[['Country_Codes', 'Min']], on='Country_Codes', how='left').rename(columns={'Min': 'Min'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"my-app\")  # Initialize the geolocator\n",
    "\n",
    "# country_codes is equal to the unique values of the 'geo\\\\TIME_PERIOD' column, filtering out EA19 and EA20\n",
    "country_codes = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] != 'EA19'][df_filtered['geo\\\\TIME_PERIOD'] != 'EA20']['geo\\\\TIME_PERIOD'].unique()\n",
    "\n",
    "data = {\"geo\\\\TIME_PERIOD\": country_codes, \"Latitude\": [], \"Longitude\": []}\n",
    "\n",
    "for country_code in country_codes:\n",
    "    location = geolocator.geocode(country_code)\n",
    "    if location is not None:\n",
    "        data[\"Latitude\"].append(location.latitude)\n",
    "        data[\"Longitude\"].append(location.longitude)\n",
    "    else:\n",
    "        data[\"Latitude\"].append(None)\n",
    "        data[\"Longitude\"].append(None)\n",
    "\n",
    "df_coords = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Merge df_filtered and df_coords\n",
    "df_filtered = df_filtered.merge(\n",
    "    df_coords[\n",
    "        [\n",
    "            'geo\\\\TIME_PERIOD', \n",
    "            'Latitude', \n",
    "            'Longitude'\n",
    "            ]\n",
    "        ],\n",
    "    left_on='geo\\\\TIME_PERIOD',\n",
    "    right_on='geo\\\\TIME_PERIOD',\n",
    "    how='left'\n",
    "    ).rename(\n",
    "        columns={\n",
    "            'Latitude': 'Latitude', \n",
    "            'Longitude': 'Longitude'\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Rename geo\\\\TIME_PERIOD to geo in df_filtered\n",
    "df_filtered = df_filtered.rename(columns={'geo\\\\TIME_PERIOD': 'geo'})\n",
    "\n",
    "# merge the comp_df and df_filtered on geo == geo and keeping all rows from comp_df\n",
    "df_filtered = comp_df.merge(df_filtered, left_on='geo', right_on='geo', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the EA19 and EA20 rows\n",
    "df_filtered = df_filtered[df_filtered[\"geo\"].isin([\"EA19\", \"EA20\"]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial figures\n",
    "fig_line = px.line(df_filtered, x=\"Quarter_x\", y=[\"Actual\", \"Predicted\"], color=\"Country_Codes\") \n",
    "fig_map = px.choropleth(df_filtered, locations=\"Country_Codes\", color=\"Value\")\n",
    "\n",
    "# Add another line to fig_line based on Predicted column\n",
    "fig_line.add_trace(go.Scatter(x=df_filtered[\"Quarter_x\"], y=df_filtered[\"Predicted\"], name=\"Predicted\", mode=\"lines\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "\n",
    "# Create app layout\n",
    "app = JupyterDash(__name__) # Use JupyterDash instead of Dash\n",
    "app.layout = html.Div([\n",
    " html.H2(\n",
    " \"Quarterly Index (Non-Seasonally Adjusted)\", \n",
    " id=\"heading\",\n",
    " style={\"textAlign\": \"center\"}\n",
    " ),\n",
    " dcc.RadioItems( \n",
    " id=\"country-buttons\", \n",
    " options=[{\"label\": c, \"value\": c} for c in df_filtered[\"Country_Names\"].unique()], # Use full country names as labels\n",
    " value=df_filtered[\"Country_Names\"].iloc[0],\n",
    " style={\"table-layout\": \"fixed\", \"width\": \"50%\"}, # Style the container element \n",
    " labelStyle={\"display\": \"inline-block\", \"margin-right\": \"10px\"} # Style each label element\n",
    " ),\n",
    " html.Div([ # Wrap line chart and map in a div with flex display\n",
    "     html.Div([ # Wrap dropdown in a div with 50% width\n",
    "         dcc.Graph(id=\"map-chart\", figure=fig_map)\n",
    "     ], style={\"width\": \"50%\"}),\n",
    "     html.Div([ # Wrap map in a div with 50% width\n",
    "         dcc.Graph(id=\"line-chart\", figure=fig_line)\n",
    "     ], style={\"width\": \"100%\"})\n",
    " ], style={\"display\": \"flex\", \"flex-direction\": \"row\"}),\n",
    "])\n",
    "\n",
    "# Define callback function\n",
    "@app.callback(\n",
    " [Output(\"line-chart\", \"figure\"), Output(\"map-chart\", \"figure\"), Output(\"heading\", \"children\")],\n",
    " [Input(\"country-buttons\", \"value\")]\n",
    ")\n",
    "\n",
    "\n",
    "def update_charts(country_name):\n",
    " # Filter dataframe by selected country name\n",
    " df_country = df_filtered[df_filtered[\"Country_Names\"] == country_name] \n",
    " # Create new figures\n",
    " fig_line = px.line(df_country, x=\"Quarter_x\", y=\"Actual\", color=\"Country_Codes\", hover_data={\"Country_Names\": True, \"Quarter_x\": True,\"Actual\": True, \"Country_Codes\": False}, labels={\"Country_Names\": \"Country\", \"Quarter_x\": \"Quarter\", \"Actual\": \"Actual\"}) \n",
    " fig_map = px.choropleth(df_country, locations=\"Country_Codes\", color=\"Value\", hover_data={\"Country_Names\": True, \"Max_y\": True,\"Min_y\": True},labels={\"Country_Names\": \"Country\", \"Max_y\": \"Max\", \"Min_y\": \"Min\"}) \n",
    " \n",
    " # Add another line to fig_line based on Value_Pred column\n",
    " fig_line.add_trace(go.Scatter(x=df_country[\"Quarter_x\"], y=df_country[\"Predicted\"], name=\"Predicted Value\", mode=\"lines\", line=dict(color=\"black\", dash=\"dash\")))\n",
    " \n",
    " # Update the hovermode and showlegend\n",
    " fig_map.update_layout(\n",
    "    hovermode='closest'\n",
    " )\n",
    " \n",
    " #Customise Map Tooltip Options\n",
    " fig_map.update_traces(hovertemplate=\"<b>%{customdata[0]}</b><br>Max: %{customdata[1]}<br>Min: %{customdata[2]}<extra></extra>\")\n",
    " \n",
    " # Use custom values for center based on latitude and longitude columns and higher value for projection.scale \n",
    " fig_map.update_layout(geo=dict(center=dict(lat=df_country['Latitude'].iloc[0], lon=df_country['Longitude'].iloc[0]), projection_scale=5))\n",
    " \n",
    " \n",
    " # Return new figures\n",
    " return fig_line, fig_map, f\"Quarterly Index (Non-Seasonally Adjusted) for {country_name}\"\n",
    "\n",
    "\n",
    "# Run app\n",
    "app.run_server(mode=\"external\") # Set mode to \"inline\" or \"external\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
