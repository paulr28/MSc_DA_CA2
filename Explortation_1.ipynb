{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import praw\n",
    "from textblob import TextBlob\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import wilcoxon\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "from jupyter_dash import JupyterDash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/EI_ISBU_Q?format=TSV&compressed=true' # The url of the data\n",
    "headers = {'Accept-Encoding': 'gzip'} # This is important to get the gzip file\n",
    "response = requests.get(url, headers=headers) # Get the data from the url\n",
    "\n",
    "buf = BytesIO(response.content) # Read the gzip file\n",
    "f = gzip.GzipFile(fileobj=buf) # Unzip the gzip file\n",
    "content = f.read() # Read the unzipped file\n",
    "\n",
    "df = pd.read_csv(BytesIO(content), sep='\\t') # Read the unzipped file as a dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Melt the dataframe so all the Quarters are in one column\n",
    "df = pd.melt(df, id_vars=['freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD'], var_name='Quarter', value_name='Value')\n",
    "\n",
    "# The column freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD is split into 6 columns\n",
    "df[['freq','unit','s_adj','indic','nace_r2','geo\\TIME_PERIOD']] = df['freq,unit,s_adj,indic,nace_r2,geo\\\\TIME_PERIOD'].str.split(',', expand=True)\n",
    "\n",
    "# Drop the column freq,unit,s_adj,indic,nace_r2,geo\\TIME_PERIOD and move Quarter and Value to the back\n",
    "df = df.drop(['freq,unit,s_adj,indic,nace_r2,geo\\\\TIME_PERIOD'], axis=1)\n",
    "df = df[['freq', 'unit', 's_adj', 'indic', 'nace_r2','geo\\TIME_PERIOD','Quarter', 'Value']]\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# set values containing \":\" to NaN\n",
    "df.loc[df['Value'].str.contains(':'), 'Value'] = pd.np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# if the Value column has an alphabetical character remove it\n",
    "df['Value'] = df['Value'].str.replace('[a-zA-Z]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Convert the Value column to a float\n",
    "df['Value'] = df['Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "countrycode_map = {c.alpha_2: c.alpha_3 for c in pycountry.countries} # Create a dictionary of country codes\n",
    "\n",
    "#Uk is not a country code so we will change it to GB\n",
    "df.loc[df['geo\\\\TIME_PERIOD'] == 'UK', 'geo\\\\TIME_PERIOD'] = 'GB'\n",
    "\n",
    "df['Country_Codes'] = df['geo\\\\TIME_PERIOD'].map(countrycode_map) # Map the country codes to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Print unique values of the Country_Codes column\n",
    "print(df['Country_Codes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Perform Normailty tests for the Value column grouped by geo\\TIME_PERIOD, indic, and s_adj\n",
    "\n",
    "df.set_index(['geo\\\\TIME_PERIOD', 'indic', 's_adj'], inplace=True)\n",
    "\n",
    "# Perform normality test on the Value column grouped by geo\\TIME_PERIOD, indic, and s_adj\n",
    "grouped = df.groupby(['geo\\\\TIME_PERIOD', 'indic', 's_adj'])['Value']\n",
    "pvalues = grouped.apply(lambda x: normaltest(x.dropna())[1])\n",
    "\n",
    "# Add p-values to a new column in the original DataFrame\n",
    "df.loc[pvalues.index, 'pvalue'] = pvalues\n",
    "\n",
    "#Add a new column to the dfi dataframe called 'normal' and set it to True if the pvalue is greater than 0.05 and False if it is less than 0.05\n",
    "df['normal'] = df['pvalue'] > 0.05\n",
    "\n",
    "# Move the index back to columns\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a country plot with the Value column as the color and the Quarter column as the animation, show for indicators '[IS-IP]' and Seasonal Adjustment 'NSA'\n",
    "\n",
    "fig = px.choropleth(df[(df['indic'] == 'IS-IP') & (df['s_adj'] == 'NSA')], locations=\"Country_Codes\",color=\"Value\", hover_name=\"geo\\TIME_PERIOD\", animation_frame=\"Quarter\", color_continuous_scale=px.colors.sequential.Plasma, range_color=(0, 100), scope='europe') # Create the plot\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by geo\\TIME_PERIOD, indic, and s_adj and show descriptive statistics\n",
    "df.groupby(['s_adj', 'indic', 'geo\\\\TIME_PERIOD']).describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the geo\\TIME_PERIOD value that equals 'IE' into a seperate dataframe\n",
    "df_ie = df[df['geo\\TIME_PERIOD'] == 'IE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform EDA on the IE dataframe\n",
    "df_ie.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ie.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Statistical Analysis on the IE dataframe\n",
    "df_ie.describe(include='all')\n",
    "\n",
    "df_ie.groupby(['indic','s_adj']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histomgrams of the Value column for the IE dataframe grouped by 'indic' and 's_adj', faceted by the 'normal' column\n",
    "sns.FacetGrid(df_ie, col='indic', row='s_adj', hue='normal').map(sns.histplot, 'Value').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms of the Value column for the IE dataframe grouped by 'indic' and 's_adj', where the 'normal' column is True\n",
    "sns.FacetGrid(df_ie[df_ie['normal'] == True], row='indic', col='s_adj', hue= 's_adj').map(sns.histplot, 'Value').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the IE dataframe showing statistical analysis using seaborn  and matplotlib  libraries splitting the data by insdicators and seasonanal adjustment\n",
    "sns.catplot(x=\"indic\", y=\"Value\", hue=\"s_adj\", kind=\"box\", data=df_ie);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the unique geo\\TIME_PERIOD values from the df dataframe, where the indic column equals 'IS-EPI' and the normal column equals True\n",
    "df[(df['indic'] == 'IS-EPI') & (df['normal'] == True)]['geo\\\\TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Ireland with 'EU28' which is the overall EU average for 28 countries\n",
    "\n",
    "# Create a dataframe called df_ie_eu28 that contains the geo\\TIME_PERIOD values of 'IE' and 'EU28'\n",
    "df_ie_eu28 = df[(df['geo\\\\TIME_PERIOD'] == 'IE') | (df['geo\\\\TIME_PERIOD'] == 'EU28')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Number of persons employed index, non-seasonally adjusted, for Ireland and EU28 using a t-test\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Filter the DataFrame to only include rows with geo\\TIME_PERIOD values of 'EU28' and 'IE',\n",
    "# 'IS-EPI' for indic, and 'NSA' for s_adj\n",
    "df_filtered = df[(df['geo\\\\TIME_PERIOD'].isin(['EU28', 'IE'])) & (df['indic'] == 'IS-EPI') & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Split the DataFrame into two separate DataFrames, one for each geo\\TIME_PERIOD value\n",
    "df_eu281 = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == 'EU28']\n",
    "df_ie1 = df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == 'IE']\n",
    "\n",
    "# Perform the t-test\n",
    "t_stat, p_val = ttest_ind(df_eu281['Value'], df_ie1['Value'], equal_var=False)\n",
    "\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a One-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include rows where the normal column equals True and the indic column equals 'IS-EPI' and the s_adj column equals 'NSA'\n",
    "df_filtered = df[(df['normal'] == True) & (df['indic'] == 'IS-EPI') & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Create a list of the unique geo\\TIME_PERIOD values\n",
    "geo_time_periods = df_filtered['geo\\\\TIME_PERIOD'].unique()\n",
    "\n",
    "#Make sure the groups have the same sample size\n",
    "min_sample_size = df_filtered['geo\\\\TIME_PERIOD'].value_counts().min()\n",
    "df_filtered = df_filtered.groupby('geo\\\\TIME_PERIOD').apply(lambda x: x.sample(min_sample_size))\n",
    "\n",
    "#Perform the ANOVA\n",
    "f_stat, p_val = f_oneway(*[df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == geo_time_period]['Value'] for geo_time_period in geo_time_periods])\n",
    "\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "#Print th resuts of the ANOVA test\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject null hypothesis - Significant differences exist between groups.\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - No significant difference between groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the diferences between the groups using a boxplot with slanted x-axis labels\n",
    "sns.catplot(x=\"geo\\\\TIME_PERIOD\", y=\"Value\", kind=\"box\", data=df_filtered).set_xticklabels(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Tukey's Range Test to determine which groups are significantly different from each other\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's Range Test\n",
    "tukey_results = pairwise_tukeyhsd(df_filtered['Value'], df_filtered['geo\\\\TIME_PERIOD'], 0.05)\n",
    "\n",
    "# Print the results\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Two-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe called prep1 where the indic column equals 'IS-EPI' and the s_adj column contains 'SCA and 'NSA' values and the normal column equals True\n",
    "prep1 = df[(df['indic'] == 'IS-EPI') & (df['normal'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the name of the geo\\TIME_PERIOD column to geo in the df dataframe\n",
    "prep1.rename(columns={'geo\\\\TIME_PERIOD': 'geo'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a TWO-WAY ANOVA to determine if there is an interaction between the geo\\TIME_PERIOD and s_adj columns\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Value ~ s_adj + geo', data = prep1).fit()\n",
    "aov2 = sm.stats.anova_lm(model, type=2)\n",
    "print(aov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Wilcoxon Signed-Rank Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find countries that have false values for the normal column where the indic column equals 'IS-HWI'\n",
    "df[(df['normal'] == False) & (df['indic'] == 'IS-HWI')]['geo\\\\TIME_PERIOD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_HWI = df[(df['indic'] == 'IS-HWI')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_HWI = df_HWI.dropna(subset=['Value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the two groups to compare\n",
    "group1 = df_HWI[df_HWI['geo\\\\TIME_PERIOD'] == 'IE']['Value']\n",
    "group2 = df_HWI[df_HWI['geo\\\\TIME_PERIOD'] == 'EU28']['Value']\n",
    "\n",
    "# Ensure the two groups have the same sample size\n",
    "min_sample_size = min(len(group1), len(group2))\n",
    "group1 = group1.sample(min_sample_size)\n",
    "group2 = group2.sample(min_sample_size)\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat, p = wilcoxon(group1, group2)\n",
    "\n",
    "# Print the results\n",
    "print('Wilcoxon signed-rank test:')\n",
    "print(f'statistic: {stat:.4f}')\n",
    "print(f'p-value: {p:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Kruskall Wallis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')]\n",
    "\n",
    "# Filter out NaN values from the 'Value' column\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "stat, p = kruskal(*[df_filtered[df_filtered['geo\\\\TIME_PERIOD'] == geo]['Value'] for geo in df_filtered['geo\\\\TIME_PERIOD'].unique()])\n",
    "\n",
    "# Print the results\n",
    "print(\"Kruskal-Wallis Test Results:\")\n",
    "print(f\"Test statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the environmental variables\n",
    "APP_NAME = getenv('APP_NAME')\n",
    "APP_ID = getenv(\"APP_ID\")\n",
    "APP_SECRET = getenv(\"APP_SECRET\")\n",
    "USERNAME = getenv('REDDIT_USERNAME')\n",
    "PASSWORD = getenv('PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up reddit API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=APP_ID,\n",
    "    client_secret=APP_SECRET,\n",
    "    user_agent=APP_NAME,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    ")\n",
    "\n",
    "# Define the subreddits and search query\n",
    "subreddits = [\"Ireland\", \"Europe\"]\n",
    "query = \"house prices\"\n",
    "\n",
    "# Collect posts from the subreddits related to the search query\n",
    "posts = []\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for post in subreddit.search(query):\n",
    "        posts.append(\n",
    "            {\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"title\": post.title,\n",
    "                \"text\": post.selftext,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the collected posts into a dataframe\n",
    "df_posts = pd.DataFrame(posts)\n",
    "\n",
    "# Perform sentiment analysis on the collected posts\n",
    "df_posts[\"polarity\"] = df_posts[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Group the results by subreddit and calculate the mean polarity\n",
    "results = df_posts.groupby(\"subreddit\")[\"polarity\"].mean()\n",
    "\n",
    "# Print the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the results of the sentiment analysis\n",
    "if results[\"Ireland\"] > results[\"Europe\"]:\n",
    "    print(\"The sentiment of the posts from r/Ireland is more positive than the sentiment of the posts from r/Europe.\")\n",
    "elif results[\"Ireland\"] < results[\"Europe\"]:\n",
    "    print(\"The sentiment of the posts from r/Ireland is more negative than the sentiment of the posts from r/Europe.\")\n",
    "else:\n",
    "    print(\"The sentiment of the posts from r/Ireland is the same as the sentiment of the posts from r/Europe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Analysis\n",
    "\n",
    "# Create a dataframe called df_filtered where the indic column equals 'IS-HWI' and the s_adj column contains 'NSA' values and the normal column equals False\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA') & (df['geo\\\\TIME_PERIOD'] == 'IE')]\n",
    "df_filtered = df_filtered.dropna(subset=['Value'])\n",
    "\n",
    "# Define a function to parse the date string\n",
    "def parse_quarter(date_string):\n",
    "    year, quarter = date_string.split('-Q')\n",
    "    month = (int(quarter) - 1) * 3 + 1\n",
    "    return datetime(int(year), month, 1)\n",
    "\n",
    "#Apply the parse_quarter function to the 'Quarter' column\n",
    "df_filtered['Quarter'] = df_filtered['Quarter'].apply(parse_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'Quarter' column as the index\n",
    "df_filtered.set_index('Quarter', inplace=True)\n",
    "\n",
    "# Create a time series plot of the 'Value' column\n",
    "df_filtered['Value'].plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Hours Worked Index in Ireland')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.asfreq('QS-OCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "# Perform time series analysis using SARIMAX\n",
    "# Split the data into train and test sets\n",
    "train = df_filtered.iloc[:len(df_filtered) - 4] # everything up to the last 4 observations\n",
    "test = df_filtered.iloc[len(df_filtered) - 4:]  # the last 4 observations\n",
    "\n",
    "# Create a SARIMAX model\n",
    "model = sm.tsa.statespace.SARIMAX(train['Value'])\n",
    "fit_model = model.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = fit_model.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions.plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions for the last 4 quarters')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test['Value'], predictions))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test['Value'], predictions, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import product\n",
    "# Tune the models hyperparameters to improve the RMSE\n",
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))\n",
    "\n",
    "# Find the optimal set of parameters that yields the best performance\n",
    "# Define the initial parameters\n",
    "best_score, best_params, best_seasonal_params = float(\"inf\"), None, None\n",
    "\n",
    "# Loop through the parameter combinations\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            # Create a SARIMAX model\n",
    "            model = sm.tsa.statespace.SARIMAX(train['Value'], order=param, seasonal_order=param_seasonal)\n",
    "\n",
    "            # Fit the model\n",
    "            results = model.fit()\n",
    "\n",
    "            # Generate predictions\n",
    "            predictions = results.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "            # Calculate the mean squared error\n",
    "            mse = mean_squared_error(test['Value'], predictions)\n",
    "\n",
    "            # If the mse is lower than our best score, update the best score, and best parameters\n",
    "            if mse < best_score:\n",
    "                best_score, best_params, best_seasonal_params = mse, param, param_seasonal\n",
    "\n",
    "            # Print the model parameters and the mean squared error\n",
    "            print('SARIMA{}x{}4 - AIC:{}'.format(param, param_seasonal, mse))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "# Print the best model parameters and the mean squared error\n",
    "print('Best SARIMA{}x{}4 AIC:{}'.format(best_params, best_seasonal_params, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SARIMAX model\n",
    "model1 = sm.tsa.statespace.SARIMAX(train['Value'], order=(1, 0, 0), seasonal_order=(0, 0, 1, 4))\n",
    "fit_model1 = model1.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions1 = fit_model1.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions1)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions1.plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions for the last 4 quarters')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test['Value'], predictions))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test['Value'], predictions, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the model\n",
    "model = sm.tsa.statespace.SARIMAX(train['Value'],order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "results = model.fit()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = fit_model.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "# Plot the predictions\n",
    "predictions.plot(figsize=(12, 5))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Predictions for the last 4 quarters')\n",
    "\n",
    "# Add a y-axis label\n",
    "plt.ylabel('Hours Worked Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the test data\n",
    "print(test)\n",
    "\n",
    "# Print the mean absolute error (MAE)\n",
    "print('The MAE is', mean_absolute_error(test['Value'], predictions))\n",
    "\n",
    "# Print the root mean squared error (RMSE)\n",
    "print('The RMSE is', mean_squared_error(test['Value'], predictions, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "results.plot_diagnostics(figsize=(12, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "pred = results.predict()\n",
    "\n",
    "# Plot the actual values and the predicted values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(df_filtered['Value'], label='Actual')\n",
    "ax.plot(pred, label='Predicted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction for the next 4 quarters\n",
    "pred = results.predict(start=len(df_filtered)-4, end=len(df_filtered) + 3)\n",
    "\n",
    "# Print the predictions\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Current Year with actual and predicted values and the Next Year with predicted values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(df_filtered['Value'], label='Actual')\n",
    "ax.plot(pred, label='Predicted')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Analysis across countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe called df_filtered where the indic column equals 'IS-HWI' and the s_adj column contains 'NSA' values and the normal column equals False\n",
    "df_filteredall = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')]\n",
    "df_filteredall = df_filteredall.dropna(subset=['Value'])\n",
    "\n",
    "# Define a function to parse the date string\n",
    "def parse_quarter(date_string):\n",
    "    year, quarter = date_string.split('-Q')\n",
    "    month = (int(quarter) - 1) * 3 + 1\n",
    "    return datetime(int(year), month, 1)\n",
    "\n",
    "#Apply the parse_quarter function to the 'Quarter' column\n",
    "df_filteredall['Quarter'] = df_filteredall['Quarter'].apply(parse_quarter)\n",
    "\n",
    "# Set the style of the plots\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Use the 'FacetGrid' function to create facetted plots\n",
    "g = sns.FacetGrid(df_filteredall, col='Country_Codes', col_wrap=3, height=4)\n",
    "\n",
    "# Map the line plot onto the facetted plots\n",
    "g.map(sns.lineplot, 'Quarter', 'Value')\n",
    "\n",
    "# Set the x-axis label for each plot\n",
    "g.set_axis_labels('Quarter', 'Value')\n",
    "\n",
    "# Set the title for each plot\n",
    "g.set_titles('{col_name}')\n",
    "\n",
    "# Adjust the spacing between the plots\n",
    "g.tight_layout()\n",
    "\n",
    "# Show the facetted plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index the dataframe by the 'Quarter' column\n",
    "df_filtered.set_index('Quarter', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sarimax_model(data):\n",
    "    # Split the data into train and test sets (e.g., 80% train, 20% test)\n",
    "    train_data = data[:-4]  # Use all but the last 4 quarters for training\n",
    "    test_data = data[-4:]  # Use the last 4 quarters for testing\n",
    "    \n",
    "    # Fit the SARIMAX model\n",
    "    model = sm.tsa.statespace.SARIMAX(train_data, order=(1, 0, 0), seasonal_order=(1, 0, 0, 4))\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Make predictions for the test set\n",
    "    predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
    "\n",
    "    # Return the model fit and predictions\n",
    "    return model_fit, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # Dictionary to store the results\n",
    "\n",
    "# Iterate over each country\n",
    "for country in df_filteredall['geo\\\\TIME_PERIOD'].unique():\n",
    "    # Filter the data for the current country and reset the index\n",
    "    country_data = df_filteredall[df_filteredall['geo\\\\TIME_PERIOD'] == country]['Value'].reset_index(drop=True)\n",
    "\n",
    "    # Apply differencing to make the series stationary\n",
    "    country_data_diff = country_data.diff().dropna()\n",
    "\n",
    "    # Fit SARIMAX model and make predictions\n",
    "    model_fit, predictions = fit_sarimax_model(country_data_diff)\n",
    "\n",
    "    # Store the model fit and predictions for the country\n",
    "    results[country] = {'model_fit': model_fit, 'predictions': predictions}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Model 2\n",
    "\n",
    "Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_drop = df.dropna(subset=['Value'])\n",
    "df_drop = df_drop[(df_drop['indic'] == 'IS-EPI')]\n",
    "\n",
    "# Scale the 'Value' column\n",
    "df_drop['Value'] = StandardScaler().fit_transform(df_drop[['Value']])\n",
    "\n",
    "# select the relevant columns as features and target\n",
    "X = df_drop[['geo\\\\TIME_PERIOD', 's_adj']]\n",
    "y = df_drop['Value']\n",
    "\n",
    "# perform one-hot encoding on the categorical columns\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2)\n",
    "\n",
    "# create and fit the model\n",
    "model = SVR()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate the model's performance metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Support Vector Regression object\n",
    "svr = SVR()\n",
    "\n",
    "# define the hyperparameter grid to search over\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "\n",
    "# create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters found\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters to create a new model\n",
    "best_svr = SVR(C=100, gamma=0.1, kernel='linear')\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_svr.predict(X_test)\n",
    "\n",
    "# Calculate the model's performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the actual values and predicted values\n",
    "plt.scatter(y_test, y_pred, color='blue')\n",
    "\n",
    "# Add a diagonal line for comparison\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('SVR Predictions vs. Actual Values')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dashboard to show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "df_filtered = df[(df['indic'] == 'IS-HWI') & (df['normal'] == False) & (df['s_adj'] == 'NSA')].copy()\n",
    "df_filtered.dropna(subset=['Value'], inplace=True)\n",
    "\n",
    "# Drop the rows with null values in Country_Codes column\n",
    "df_filtered = df_filtered.dropna(subset=[\"Country_Codes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column to df_filtered called Value_Pred, with is the Value column + 10 - This will later be the predicted time series values.\n",
    "df_filtered[\"Value_Pred\"] = df_filtered[\"Value\"] + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map country codes to full names\n",
    "country_names = {\n",
    " \"NLD\": \"Netherlands\",\n",
    " \"LTU\": \"Lithuania\",\n",
    " \"BEL\": \"Belgium\",\n",
    " \"BGR\": \"Bulgaria\",\n",
    " \"CYP\": \"Cyprus\",\n",
    " \"EST\": \"Estonia\",\n",
    " \"ESP\": \"Spain\",\n",
    " \"FIN\": \"Finland\",\n",
    " \"HRV\": \"Croatia\",\n",
    " \"IRL\": \"Ireland\",\n",
    " \"ITA\": \"Italy\",\n",
    " \"LVA\": \"Latvia\",\n",
    " \"MNE\": \"Montenegro\",\n",
    " \"MLT\": \"Malta\",\n",
    " \"NOR\": \"Norway\",\n",
    " \"PRT\": \"Portugal\",\n",
    " \"SWE\": \"Sweden\",\n",
    " \"TUR\": \"Turkey\",\n",
    " \"MKD\": \"North Macedonia\"\n",
    "}\n",
    "\n",
    "# Create a new column in df_filtered with full country names\n",
    "df_filtered[\"Country_Names\"] = df_filtered[\"Country_Codes\"].map(country_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#Create dashboard layout\n",
    "# Create initial figures\n",
    "fig_line = px.line(df_filtered, x=\"Quarter\", y=\"Value\", color=\"Country_Codes\") \n",
    "fig_map = px.choropleth(df_filtered, locations=\"Country_Codes\", color=\"Value\", scope=\"europe\") \n",
    "\n",
    "# Add another line to fig_line based on Value_Pred column\n",
    "fig_line.add_trace(go.Scatter(x=df_filtered[\"Quarter\"], y=df_filtered[\"Value_Pred\"], name=\"Predicted Value\", mode=\"lines\", line=dict(color=\"black\", dash=\"dash\")))\n",
    "\n",
    "# Create app layout\n",
    "app = JupyterDash(__name__) # Use JupyterDash instead of Dash\n",
    "app.layout = html.Div([\n",
    " html.H1(\"Dashboard Example\"),\n",
    " dcc.RadioItems( \n",
    " id=\"country-buttons\", \n",
    " options=[{\"label\": c, \"value\": c} for c in df_filtered[\"Country_Names\"].unique()], # Use full country names as labels\n",
    " value=df_filtered[\"Country_Names\"].iloc[0],\n",
    " style={\"table-layout\": \"auto\"}, # Style the container element \n",
    " labelStyle={\"display\": \"inline-block\", \"margin-right\": \"10px\"} # Style each label element\n",
    " ),\n",
    " html.Div([ # Wrap line chart and map in a div with flex display\n",
    "     html.Div([ # Wrap dropdown in a div with 50% width\n",
    "         dcc.Graph(id=\"map-chart\", figure=fig_map)\n",
    "     ], style={\"width\": \"50%\"}),\n",
    "     html.Div([ # Wrap map in a div with 50% width\n",
    "         dcc.Graph(id=\"line-chart\", figure=fig_line)\n",
    "     ], style={\"width\": \"50%\"})\n",
    " ], style={\"display\": \"flex\", \"flex-direction\": \"row\"}),\n",
    "])\n",
    "\n",
    "# Define callback function\n",
    "@app.callback(\n",
    " [Output(\"line-chart\", \"figure\"), Output(\"map-chart\", \"figure\")],\n",
    " [Input(\"country-buttons\", \"value\")]\n",
    ")\n",
    "def update_charts(country_name):\n",
    " # Filter dataframe by selected country name\n",
    " df_country = df_filtered[df_filtered[\"Country_Names\"] == country_name] \n",
    " # Create new figures\n",
    " fig_line = px.line(df_country, x=\"Quarter\", y=\"Value\", color=\"Country_Codes\") \n",
    " fig_map = px.choropleth(df_country, locations=\"Country_Codes\", color=\"Value\", scope=\"europe\") \n",
    " # Add another line to fig_line based on Value_Pred column\n",
    " fig_line.add_trace(go.Scatter(x=df_country[\"Quarter\"], y=df_country[\"Value_Pred\"], name=\"Predicted Value\", mode=\"lines\", line=dict(color=\"black\", dash=\"dash\")))\n",
    " # Return new figures\n",
    " return fig_line, fig_map\n",
    "\n",
    "# Run app\n",
    "app.run_server(mode=\"external\") # Set mode to \"inline\" or \"external\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
